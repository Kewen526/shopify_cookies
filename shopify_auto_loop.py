# -*- coding: utf-8 -*-
"""
Shopify CSV è‡ªåŠ¨ç”Ÿæˆ + ä¸Šä¼  + æ—¥å¿—ç»Ÿè®¡
åŠŸèƒ½ï¼š24å°æ—¶æ— é™å¾ªç¯ï¼Œæ¯3åˆ†é’Ÿå¤„ç†ä¸€ä¸ªäº§å“
æ—¥å¿—ç›®å½•ï¼šC:\ShopifyAutoLog\
"""

import csv
import json
import os
import re
import time
import threading
import uuid
import requests
import pymysql
from selenium import webdriver
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.chrome.service import Service as ChromeService

from datetime import datetime
from typing import Optional, List, Dict
from dataclasses import dataclass, field
from urllib import parse
from pathlib import Path


# ============================================================
# å…¨å±€é…ç½®
# ============================================================

# ç¦ç”¨ç³»ç»Ÿä»£ç†
os.environ['NO_PROXY'] = '*'
os.environ['no_proxy'] = '*'
for _k in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
    os.environ.pop(_k, None)

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    "host": "47.95.157.46",
    "user": "root",
    "password": "root@kunkun",
    "port": 3306,
    "database": "quote_iw",
    "charset": "utf8mb4"
}

# APIåŸºç¡€åœ°å€
API_BASE_URL     = "http://47.95.157.46:8520"
LOG_API_BASE_URL = "http://47.104.72.198:2580"   # æ—¥å¿— & Cookie çŠ¶æ€ API

# Shopifyé…ç½®
STORE_ID = "893848-2"
COOKIE_URL = "https://ceshi-1300392622.cos.ap-beijing.myqcloud.com/shopify-cookies/893848-2.json"

# æ—¥å¿—ç›®å½•
LOG_DIR = r"C:\ShopifyAutoLog"

# æ‰§è¡Œé—´éš”ï¼ˆç§’ï¼‰
TASK_INTERVAL_SECONDS = 180   # æ¯3åˆ†é’Ÿä¸€ä¸ªäº§å“
NO_TASK_WAIT_SECONDS  = 30    # æ— ä»»åŠ¡æ—¶ç­‰å¾…30ç§’


# ============================================================
# æ—¥å¿—å‡½æ•°
# ============================================================

def log_info(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] INFO: {msg}")

def log_warning(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] WARNING: {msg}")

def log_error(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ERROR: {msg}")


# ============================================================
# æ¯æ—¥ç»Ÿè®¡æ—¥å¿—
# ============================================================

def _ensure_log_dir():
    os.makedirs(LOG_DIR, exist_ok=True)

def _today_log_path() -> str:
    _ensure_log_dir()
    date_str = datetime.now().strftime('%Y-%m-%d')
    return os.path.join(LOG_DIR, f"shopify_{date_str}.log")

def write_daily_log(keer_product_id: str, result: str, detail: str = ""):
    """
    å‘å½“å¤©æ—¥å¿—æ–‡ä»¶è¿½åŠ ä¸€æ¡è®°å½•ï¼Œå¹¶åŒæ­¥å†™å…¥æ•°æ®åº“
    result: 'success' / 'failed' / 'skipped'
    """
    # å†™æœ¬åœ°æ–‡ä»¶ï¼ˆåŸæœ‰é€»è¾‘ä¿ç•™ï¼‰
    log_path = _today_log_path()
    now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    line = f"[{now_str}] [{result.upper():8s}] ID={str(keer_product_id or '-'):30s} {detail}\n"
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(line)

    # åŒæ­¥å†™å…¥æ•°æ®åº“
    _write_db_log(keer_product_id, result, detail)


def _write_db_log(keer_product_id: str, result: str, detail: str = ""):
    """å°†ä»»åŠ¡æ‰§è¡Œç»“æœå†™å…¥ shopify_task_log è¡¨"""
    try:
        conn = pymysql.connect(**DB_CONFIG)
        try:
            with conn.cursor() as cursor:
                sql = """
                    INSERT INTO shopify_task_log
                        (task_date, keer_product_id, result, detail, created_at)
                    VALUES (%s, %s, %s, %s, %s)
                """
                now = datetime.now()
                cursor.execute(sql, (
                    now.strftime('%Y-%m-%d'),
                    keer_product_id or '',
                    result,
                    detail[:500] if detail else '',
                    now.strftime('%Y-%m-%d %H:%M:%S')
                ))
            conn.commit()
        finally:
            conn.close()
    except Exception as e:
        log_error(f"DBæ—¥å¿—å†™å…¥å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

def write_daily_summary():
    """
    åœ¨æ—¥å¿—æ–‡ä»¶æœ«å°¾è¿½åŠ å½“å¤©çš„æ±‡æ€»ç»Ÿè®¡ï¼ˆæ¯æ¬¡å¾ªç¯éƒ½æ›´æ–°æœ«å°¾æ±‡æ€»è¡Œï¼‰
    æ”¹ä¸ºæ¯æ¬¡ä»»åŠ¡ç»“æŸåè°ƒç”¨ï¼Œç»Ÿè®¡å½“å¤©æ–‡ä»¶å†…å®¹ã€‚
    """
    log_path = _today_log_path()
    if not os.path.exists(log_path):
        return

    total = success = failed = skipped = 0
    lines = []
    with open(log_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # è¿‡æ»¤æ‰æ—§çš„æ±‡æ€»è¡Œï¼Œé‡æ–°ç»Ÿè®¡
    data_lines = [l for l in lines if not l.startswith('===')]
    for line in data_lines:
        if '[SUCCESS' in line:
            total += 1; success += 1
        elif '[FAILED' in line:
            total += 1; failed += 1
        elif '[SKIPPED' in line:
            skipped += 1

    summary = (
        f"{'=' * 70}\n"
        f"  å½“æ—¥æ±‡æ€» ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')} æ›´æ–°)\n"
        f"  æ‰§è¡Œä»»åŠ¡: {total}  æˆåŠŸ: {success}  å¤±è´¥: {failed}  è·³è¿‡(æ— ä»»åŠ¡): {skipped}\n"
        f"{'=' * 70}\n"
    )

    with open(log_path, 'w', encoding='utf-8') as f:
        f.writelines(data_lines)
        f.write(summary)


# ============================================================
# æ•°æ®ç±»
# ============================================================

@dataclass
class ProductVariant:
    id: int = 0
    title: str = ""
    price: str = "0"
    compare_at_price: Optional[str] = None
    sku: str = ""
    available: bool = True
    option1: Optional[str] = None
    option2: Optional[str] = None
    option3: Optional[str] = None
    grams: int = 0

@dataclass
class ProductImage:
    id: int = 0
    src: str = ""
    alt: Optional[str] = None
    position: int = 0

@dataclass
class ProductDetail:
    id: int = 0
    title: str = ""
    handle: str = ""
    description: str = ""
    vendor: str = ""
    product_type: str = ""
    tags: List[str] = field(default_factory=list)
    variants: List[ProductVariant] = field(default_factory=list)
    images: List[ProductImage] = field(default_factory=list)
    options: List[Dict] = field(default_factory=list)


# ============================================================
# Shopifyå•†å“æŠ“å–
# ============================================================

class ShopifyScraper:
    def __init__(self, timeout: int = 15):
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json'
        })
        self.session.trust_env = False

    def fetch(self, product_url: str) -> Optional[ProductDetail]:
        is_preview = 'preview_key=' in product_url or 'products_preview' in product_url
        if is_preview:
            if '?' in product_url:
                base_url = product_url.split('?')[0]
                params = product_url.split('?')[1]
                if not base_url.endswith('.json'):
                    base_url = base_url.rstrip('/') + '.json'
                json_url = f"{base_url}?{params}"
            else:
                json_url = product_url.rstrip('/') + '.json'
        else:
            json_url = product_url.split('?')[0].rstrip('/')
            if not json_url.endswith('.json'):
                json_url += '.json'

        try:
            log_info(f"æ­£åœ¨è®¿é—®: {json_url}")
            response = self.session.get(json_url, timeout=self.timeout)
            if response.status_code == 404:
                log_error(f"å•†å“ä¸å­˜åœ¨(404): {product_url}")
                return None
            response.raise_for_status()
            data = response.json()
            if 'product' not in data:
                log_error("å“åº”ä¸­æ²¡æœ‰productå­—æ®µ")
                return None
            return self._parse(data['product'])
        except Exception as e:
            log_error(f"æŠ“å–é”™è¯¯: {e}")
            return None

    def _parse(self, data: Dict) -> ProductDetail:
        variants = []
        for v in data.get('variants', []):
            variants.append(ProductVariant(
                id=v.get('id', 0), title=v.get('title', ''),
                price=v.get('price', '0'), compare_at_price=v.get('compare_at_price'),
                sku=v.get('sku', ''), available=v.get('available', True),
                option1=v.get('option1'), option2=v.get('option2'),
                option3=v.get('option3'), grams=v.get('grams', 0)
            ))
        images = []
        for img in data.get('images', []):
            images.append(ProductImage(
                id=img.get('id', 0), src=img.get('src', ''),
                alt=img.get('alt'), position=img.get('position', 0)
            ))
        tags = data.get('tags', '')
        if isinstance(tags, str):
            tags = [t.strip() for t in tags.split(',') if t.strip()]
        return ProductDetail(
            id=data.get('id', 0), title=data.get('title', ''),
            handle=data.get('handle', ''), description=data.get('body_html', ''),
            vendor=data.get('vendor', ''), product_type=data.get('product_type', ''),
            tags=tags, variants=variants, images=images,
            options=data.get('options', [])
        )


# ============================================================
# ZhipuAI APIå¯†é’¥ç®¡ç†
# ============================================================

_global_zhipuai_keys = []
_keys_cache_lock = threading.Lock()


def _fetch_api_keys_with_retry(api_url: str, max_retries: int = 3) -> list:
    headers = {'Content-Type': 'application/x-www-form-urlencoded'}
    data = parse.urlencode({}, True)
    proxies = {'http': None, 'https': None}
    for attempt in range(1, max_retries + 1):
        try:
            log_info(f"ğŸ“¡ æ­£åœ¨è·å–ZhipuAIå¯†é’¥ (ç¬¬{attempt}/{max_retries}æ¬¡)...")
            response = requests.post(api_url, headers=headers, data=data, timeout=10, proxies=proxies)
            if response.status_code == 200:
                result = response.json()
                if result.get("success") and "data" in result:
                    keys = [item["key"].strip() for item in result["data"]]
                    log_info(f"âœ… æˆåŠŸè·å– {len(keys)} ä¸ªZhipuAI APIå¯†é’¥")
                    return keys
        except requests.exceptions.Timeout:
            log_warning(f"âš ï¸ å¯†é’¥è·å–è¶…æ—¶ (ç¬¬{attempt}/{max_retries}æ¬¡)")
            if attempt < max_retries: time.sleep(2 ** attempt)
        except Exception as e:
            log_error(f"âŒ å¯†é’¥è·å–å¼‚å¸¸: {e}")
            if attempt < max_retries: time.sleep(2 ** attempt)
    return []


def init_global_api_keys() -> bool:
    global _global_zhipuai_keys
    _global_zhipuai_keys = _fetch_api_keys_with_retry(f'{API_BASE_URL}/api/zhipuai_key', max_retries=3)
    log_info(f"ğŸ”‘ ZhipuAIå¯†é’¥ç¼“å­˜åˆå§‹åŒ–å®Œæˆ ({len(_global_zhipuai_keys)} ä¸ª)")
    return len(_global_zhipuai_keys) > 0


def get_cached_zhipuai_keys() -> list:
    with _keys_cache_lock:
        return _global_zhipuai_keys.copy()


def refresh_api_keys() -> bool:
    global _global_zhipuai_keys
    with _keys_cache_lock:
        keys = _fetch_api_keys_with_retry(f'{API_BASE_URL}/api/zhipuai_key', max_retries=3)
        if keys:
            _global_zhipuai_keys = keys
            return True
        return False


class APIKeyManager:
    def __init__(self, blacklist_duration=180):
        self.blacklist_duration = blacklist_duration
        self.blacklisted_keys = {}
        self.lock = threading.RLock()
        self.last_used_index = -1

    def add_to_blacklist(self, api_key: str, reason: str = "è°ƒç”¨å¤±è´¥"):
        with self.lock:
            self.blacklisted_keys[api_key] = time.time()
            log_warning(f"âš ï¸ å¯†é’¥åŠ å…¥é»‘åå•({reason}): ...{api_key[-8:]} ({self.blacklist_duration}ç§’)")

    def is_blacklisted(self, api_key: str) -> bool:
        with self.lock:
            if api_key not in self.blacklisted_keys:
                return False
            if time.time() - self.blacklisted_keys[api_key] >= self.blacklist_duration:
                del self.blacklisted_keys[api_key]
                return False
            return True

    def get_next_available_key(self, all_keys: list) -> Optional[str]:
        with self.lock:
            if not all_keys:
                return None
            expired = [k for k, t in self.blacklisted_keys.items()
                       if time.time() - t >= self.blacklist_duration]
            for k in expired:
                del self.blacklisted_keys[k]
            available = [k for k in all_keys if not self.is_blacklisted(k)]
            if not available:
                log_error("âŒ æ‰€æœ‰å¯†é’¥éƒ½åœ¨é»‘åå•ä¸­ï¼")
                return None
            self.last_used_index = (self.last_used_index + 1) % len(available)
            selected = available[self.last_used_index]
            log_info(f"ğŸ”‘ è½®æ¢å¯†é’¥[{self.last_used_index + 1}/{len(available)}]: ...{selected[-8:]}")
            return selected

    def record_success(self, api_key: str):
        pass

    def record_failure(self, api_key: str, error_msg: str = ""):
        if any(kw in error_msg.lower() for kw in ['rate limit', 'too many', '429', 'å¹¶å‘', 'é™æµ']):
            self.add_to_blacklist(api_key, "é™æµ")


api_key_manager = APIKeyManager(blacklist_duration=180)


def zhipu_single_image_analyze_sync(image_url: str, prompt: str, max_retries: int = 10) -> str:
    wait_all_blocked = 30
    for attempt in range(max_retries):
        selected_key = None
        try:
            api_keys = get_cached_zhipuai_keys()
            if not api_keys:
                if refresh_api_keys():
                    api_keys = get_cached_zhipuai_keys()
            if not api_keys:
                raise Exception("æ— æ³•è·å–ZhipuAI APIå¯†é’¥")

            selected_key = api_key_manager.get_next_available_key(api_keys)
            if not selected_key:
                log_warning(f"âš ï¸ æ‰€æœ‰å¯†é’¥åœ¨é»‘åå•ï¼Œç­‰å¾…{wait_all_blocked}ç§’...")
                time.sleep(wait_all_blocked)
                selected_key = api_key_manager.get_next_available_key(api_keys)
                if not selected_key:
                    continue

            url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
            headers = {"Authorization": f"Bearer {selected_key}", "Content-Type": "application/json"}
            payload = {
                "model": "glm-4.1v-thinking-flash",
                "max_tokens": 16384,
                "top_p": 0.1,
                "messages": [{"role": "user", "content": [
                    {"type": "image_url", "image_url": {"url": image_url}},
                    {"type": "text", "text": prompt}
                ]}]
            }
            response = requests.post(url, headers=headers, json=payload, timeout=120)
            if response.status_code == 200:
                result = response.json()
                if result and 'choices' in result and result['choices']:
                    content = result['choices'][0].get('message', {}).get('content', '')
                    if content:
                        api_key_manager.record_success(selected_key)
                        return content
                raise Exception(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}, {response.text}")
        except Exception as e:
            error_msg = str(e)
            log_error(f"âŒ ZhipuAIåˆ†æå¤±è´¥(ç¬¬{attempt+1}æ¬¡): {error_msg}")
            if selected_key:
                api_key_manager.record_failure(selected_key, error_msg)
            sleep_t = 5 if '429' in error_msg else (2 if 'timeout' in error_msg.lower() else 0.5)
            time.sleep(sleep_t)
    return "åˆ†æå¤±è´¥ï¼šè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"


class ZhipuImageAnalyzer:
    def __init__(self):
        self._initialized = False

    def _ensure_initialized(self):
        if not self._initialized:
            if not get_cached_zhipuai_keys():
                init_global_api_keys()
            self._initialized = True

    def analyze(self, image_url: str, prompt: str) -> str:
        self._ensure_initialized()
        return zhipu_single_image_analyze_sync(image_url, prompt)


# ============================================================
# AIåˆ†ç±»
# ============================================================

def get_product_category(analyzer: ZhipuImageAnalyzer, image_url: str) -> Optional[str]:
    try:
        prompt = """Identify the product in the image and return the most appropriate category from this list:

1. Apparel & Accessories (clothing, shoes, jewelry, watches, hats, scarves)
2. Luggage & Bags (handbags, backpacks, wallets, suitcases)
3. Animals & Pet Supplies (pet food, pet toys, pet accessories)
4. Home & Garden (kitchen, bedding, lighting, garden tools, home decor)
5. Furniture (tables, chairs, sofas, beds, desks)
6. Electronics (phones, computers, audio, TV, smart devices)
7. Cameras & Optics (cameras, lenses, binoculars)
8. Health & Beauty (skincare, makeup, hair care, personal care)
9. Sporting Goods (fitness, outdoor, cycling, sports equipment)
10. Toys & Games (toys, games, puzzles)
11. Baby & Toddler (baby products, strollers, baby clothing)
12. Office Supplies (stationery, office equipment)
13. Vehicles & Parts (car accessories, auto parts)
14. Food, Beverages & Tobacco (food, drinks, snacks)
15. Hardware (tools, building materials)
16. Arts & Entertainment (art supplies, musical instruments)
17. Media (books, music, movies)

Rules:
1. Return ONLY the category name exactly as shown (e.g., "Apparel & Accessories")
2. Choose the single best matching category
3. No explanation, no punctuation, just the category name"""

        result = analyzer.analyze(image_url, prompt)
        if result and 'åˆ†æå¤±è´¥' not in result:
            category = result.strip()
            category = re.sub(r'<\|[^|]+\|>', '', category)
            category = re.sub(r'<think>.*?</think>', '', category, flags=re.IGNORECASE | re.DOTALL)
            category = re.sub(r'<[^>]+>', '', category)
            category = category.strip().split('\n')[0].strip()
            if len(category) > 200:
                category = category[:200]
            log_info(f"ğŸ·ï¸ AIåˆ†ç±»ç»“æœ: {category}")
            return category if category else None
        return None
    except Exception as e:
        log_error(f"âŒ AIåˆ†ç±»å¼‚å¸¸: {e}")
        return None


# ============================================================
# æ•°æ®åº“æ“ä½œ
# ============================================================

def fetch_one_task() -> Optional[Dict]:
    conn = pymysql.connect(**DB_CONFIG)
    try:
        with conn.cursor(pymysql.cursors.DictCursor) as cursor:
            sql = """
                SELECT keer_product_id, client_product_url, client_product_image,
                       quotation_result, created_at
                FROM quotation_task_detail
                WHERE created_at >= DATE_SUB(NOW(), INTERVAL 3 DAY)
                  AND task_status = 'æŠ¥ä»·å•åˆ›å»ºå®Œæ¯•'
                  AND client_product_url LIKE 'http%'
                  AND (shopfiy_task IS NULL OR shopfiy_task = '')
                  AND client_product_image IS NOT NULL
                  AND client_product_image != ''
                ORDER BY created_at DESC
                LIMIT 1
            """
            cursor.execute(sql)
            return cursor.fetchone()
    finally:
        conn.close()


def parse_price_from_quotation(quotation_result: str) -> Optional[float]:
    try:
        if not quotation_result:
            return None
        data = json.loads(quotation_result)
        if not isinstance(data, list) or not data:
            return None
        qty1_items = [item for item in data if item.get('quantity') == 1]
        if not qty1_items:
            return None
        for item in qty1_items:
            if item.get('nation') == 'US':
                return float(item.get('price', 0))
        return float(qty1_items[0].get('price', 0))
    except Exception as e:
        log_error(f"ä»·æ ¼è§£æé”™è¯¯: {e}")
        return None


def feedback_task_status(keer_product_id: str, shopfiy_task: int) -> bool:
    try:
        url = f'{API_BASE_URL}/api/task-data/save'
        payload = {"keer_product_id": keer_product_id, "shopfiy_task": shopfiy_task}
        response = requests.post(url, headers={'Content-Type': 'application/json'},
                                 json=payload, timeout=10)
        if response.status_code == 200:
            log_info(f"âœ… çŠ¶æ€åé¦ˆ: {keer_product_id} -> {'æˆåŠŸ' if shopfiy_task == 1 else 'å¤±è´¥'}")
            return True
        log_error(f"çŠ¶æ€åé¦ˆå¤±è´¥: {response.status_code}")
        return False
    except Exception as e:
        log_error(f"çŠ¶æ€åé¦ˆå¼‚å¸¸: {e}")
        return False


# ============================================================
# CSVç”Ÿæˆï¼ˆShopify 2024æ ¼å¼ï¼‰
# ============================================================

def generate_shopify_csv(product: ProductDetail, price: float, category: str,
                         output_path: str) -> bool:
    headers = [
        'Title', 'URL handle', 'Description', 'Vendor', 'Product category', 'Type', 'Tags',
        'Published on online store', 'Status',
        'SKU', 'Barcode',
        'Option1 name', 'Option1 value', 'Option1 Linked To',
        'Option2 name', 'Option2 value', 'Option2 Linked To',
        'Option3 name', 'Option3 value', 'Option3 Linked To',
        'Price', 'Compare-at price', 'Cost per item',
        'Charge tax', 'Tax code',
        'Inventory tracker', 'Inventory quantity', 'Continue selling when out of stock',
        'Weight value (grams)', 'Weight unit for display', 'Requires shipping', 'Fulfillment service',
        'Product image URL', 'Image position', 'Image alt text', 'Variant image URL',
        'Gift card', 'SEO title', 'SEO description'
    ]
    rows = []
    handle = product.handle or re.sub(r'[^a-z0-9]+', '-', product.title.lower()).strip('-')
    safe_category = (category or '')[:200]
    option1_name = product.options[0].get('name', 'Title') if product.options else 'Title'
    option2_name = product.options[1].get('name', '') if len(product.options) > 1 else ''
    option3_name = product.options[2].get('name', '') if len(product.options) > 2 else ''

    for i, variant in enumerate(product.variants):
        row = {
            'Title': product.title if i == 0 else '',
            'URL handle': handle,
            'Description': product.description if i == 0 else '',
            'Vendor': product.vendor if i == 0 else '',
            'Product category': safe_category if i == 0 else '',
            'Type': safe_category if i == 0 else '',
            'Tags': ', '.join(product.tags) if i == 0 else '',
            'Published on online store': 'TRUE' if i == 0 else '',
            'Status': 'active' if i == 0 else '',
            'SKU': variant.sku or '',
            'Barcode': '',
            'Option1 name': option1_name if i == 0 else '',
            'Option1 value': variant.option1 or 'Default Title',
            'Option1 Linked To': '',
            'Option2 name': option2_name if i == 0 else '',
            'Option2 value': variant.option2 or '',
            'Option2 Linked To': '',
            'Option3 name': option3_name if i == 0 else '',
            'Option3 value': variant.option3 or '',
            'Option3 Linked To': '',
            'Price': str(price),
            'Compare-at price': variant.compare_at_price or '',
            'Cost per item': '',
            'Charge tax': 'TRUE',
            'Tax code': '',
            'Inventory tracker': 'shopify',
            'Inventory quantity': '100',
            'Continue selling when out of stock': 'DENY',
            'Weight value (grams)': str(variant.grams) if variant.grams else '0',
            'Weight unit for display': 'g',
            'Requires shipping': 'TRUE',
            'Fulfillment service': 'manual',
            'Product image URL': product.images[0].src if i == 0 and product.images else '',
            'Image position': '1' if i == 0 and product.images else '',
            'Image alt text': product.images[0].alt or '' if i == 0 and product.images else '',
            'Variant image URL': '',
            'Gift card': 'FALSE' if i == 0 else '',
            'SEO title': product.title[:70] if i == 0 else '',
            'SEO description': ''
        }
        rows.append(row)

    for img_idx, img in enumerate(product.images[1:], start=2):
        row = {h: '' for h in headers}
        row['URL handle'] = handle
        row['Product image URL'] = img.src
        row['Image position'] = str(img_idx)
        row['Image alt text'] = img.alt or ''
        rows.append(row)

    try:
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            writer.writerows(rows)
        log_info(f"CSVæ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
        return True
    except Exception as e:
        log_error(f"CSVå†™å…¥å¤±è´¥: {e}")
        return False


# ============================================================
# Cookie çŠ¶æ€ä¸ŠæŠ¥
# ============================================================

def _report_cookie_status_worker(is_valid: bool, detail: str):
    """åå°çº¿ç¨‹ï¼šæ‰§è¡Œä¸ŠæŠ¥ï¼Œå¤±è´¥é™é»˜å¤„ç†"""
    try:
        url = f"{LOG_API_BASE_URL}/api/shopify/cookie-status/report"
        payload = {
            "store_id": STORE_ID,
            "is_valid": is_valid,
            "checker":  "auto_loop",
            "detail":   detail[:500] if detail else "",
        }
        resp = requests.post(url, json=payload, timeout=2)
        if resp.status_code == 200:
            status_str = "æœ‰æ•ˆ" if is_valid else "å¤±æ•ˆ"
            log_info(f"CookieçŠ¶æ€å·²ä¸ŠæŠ¥: {status_str} | {detail}")
        else:
            log_warning(f"CookieçŠ¶æ€ä¸ŠæŠ¥å¤±è´¥: HTTP {resp.status_code}")
    except Exception as e:
        log_warning(f"CookieçŠ¶æ€ä¸ŠæŠ¥å¼‚å¸¸ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")


def report_cookie_status(is_valid: bool, detail: str = ""):
    """
    å‘ API æœåŠ¡ä¸ŠæŠ¥å½“å‰ Cookie æœ‰æ•ˆæ€§ã€‚
    åå°çº¿ç¨‹å‘é€ï¼Œä¸é˜»å¡ä¸»æµç¨‹ã€‚
    """
    t = threading.Thread(target=_report_cookie_status_worker, args=(is_valid, detail), daemon=True)
    t.start()


# ============================================================
# Cookieä¸‹è½½ï¼ˆä»è…¾è®¯äº‘COSï¼‰
# ============================================================

def download_cookies() -> Optional[list]:
    """ä»COSä¸‹è½½Cookie JSONï¼Œè¿”å›å®Œæ•´cookieåˆ—è¡¨ï¼ˆå«domain/pathä¿¡æ¯ï¼‰"""
    try:
        log_info(f"æ­£åœ¨ä¸‹è½½Cookie: {COOKIE_URL}")
        resp = requests.get(COOKIE_URL, timeout=15)
        resp.raise_for_status()
        data = resp.json()

        cookie_list = []
        if isinstance(data, dict) and 'cookies' in data:
            for c in data['cookies']:
                if 'name' in c and 'value' in c:
                    cookie_list.append(c)
        elif isinstance(data, list):
            for c in data:
                if 'name' in c and 'value' in c:
                    cookie_list.append(c)

        log_info(f"âœ… CookieåŠ è½½æˆåŠŸï¼Œå…± {len(cookie_list)} ä¸ª")
        return cookie_list
    except Exception as e:
        log_error(f"âŒ Cookieä¸‹è½½å¤±è´¥: {e}")
        return None


# ============================================================
# Shopify CSVä¸Šä¼ 
# ============================================================

def _get_csrf_token_selenium(cookie_list: list) -> Optional[str]:
    """
    ä½¿ç”¨ Seleniumï¼ˆçœŸå® Chromeï¼‰è·å– CSRF Tokenã€‚
    requests ç›´æ¥è®¿é—®ä¼šè¢« Shopify Bot æ£€æµ‹è¿”å› 403ï¼›
    Selenium æºå¸¦å®Œæ•´æµè§ˆå™¨æŒ‡çº¹ï¼Œå¯ç»•è¿‡æ£€æµ‹ã€‚
    """
    url = f"https://admin.shopify.com/store/{STORE_ID}/products?selectedView=all"
    driver = None
    try:
        chrome_options = ChromeOptions()
        chrome_options.add_argument('--headless=new')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--window-size=1280,800')
        chrome_options.add_argument('--lang=zh-CN')
        chrome_options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
            '(KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'
        )
        # éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)

        driver = webdriver.Chrome(options=chrome_options)
        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
            'source': "Object.defineProperty(navigator, 'webdriver', {get: () => undefined});"
        })

        # Selenium æ³¨å…¥ cookie å‰å¿…é¡»å…ˆæ‰“å¼€åŒåŸŸé¡µé¢
        log_info("ğŸŒ Selenium æ­£åœ¨åŠ è½½ Shopify åå°...")
        driver.get("https://admin.shopify.com/")
        time.sleep(1)

        # æ³¨å…¥ cookie
        for c in cookie_list:
            cookie_entry = {
                'name':   c['name'],
                'value':  c['value'],
                'domain': c.get('domain', '.shopify.com'),
                'path':   c.get('path', '/'),
            }
            if c.get('secure'):
                cookie_entry['secure'] = True
            if c.get('httpOnly'):
                cookie_entry['httpOnly'] = True
            try:
                driver.add_cookie(cookie_entry)
            except Exception:
                pass  # è·³è¿‡ä¸ªåˆ«ä¸å…¼å®¹çš„ cookie

        # è®¿é—®ç›®æ ‡é¡µé¢
        driver.get(url)
        # ç­‰å¾…é¡µé¢åŠ è½½
        time.sleep(5)

        content = driver.page_source

        pattern = r'<script type="text/json" data-serialized-id="server-data">\s*(\{.*?\})\s*</script>'
        match = re.search(pattern, content, re.DOTALL)
        if not match:
            current_url = driver.current_url
            log_error(f"æœªæ‰¾åˆ° server-dataï¼Œå½“å‰URL: {current_url}")
            if 'login' in current_url or 'accounts.shopify.com' in current_url:
                report_cookie_status(False, "è¢«é‡å®šå‘è‡³ç™»å½•é¡µï¼ŒCookie å·²å¤±æ•ˆ")
            else:
                report_cookie_status(False, "Selenium æœªæ‰¾åˆ° server-dataï¼ŒCookie å¯èƒ½å·²å¤±æ•ˆ")
            return None

        server_data = json.loads(match.group(1))
        token = server_data.get('csrfToken')
        if token:
            log_info(f"âœ… Selenium è·å– CSRF token æˆåŠŸ: {token[:30]}...")
            report_cookie_status(True, "Selenium CSRF token è·å–æˆåŠŸï¼ŒCookie æœ‰æ•ˆ")
            return token

        log_error("server-data ä¸­æ—  csrfToken å­—æ®µ")
        report_cookie_status(False, "server-data ä¸­æ—  csrfToken å­—æ®µï¼ŒCookie å¯èƒ½å·²å¤±æ•ˆ")
        return None

    except Exception as e:
        log_error(f"Selenium è·å– CSRF token å¼‚å¸¸: {e}")
        report_cookie_status(False, f"Selenium å¼‚å¸¸: {str(e)[:200]}")
        return None
    finally:
        if driver:
            try:
                driver.quit()
            except Exception:
                pass


def upload_csv_to_shopify(csv_file: str) -> bool:
    """
    å°†CSVä¸Šä¼ åˆ°Shopifyåå°ï¼ˆå¤±è´¥è‡ªåŠ¨é‡è¯•1æ¬¡ï¼‰
    """
    for attempt in range(1, 3):  # æœ€å¤š2æ¬¡ï¼ˆåŸå§‹ + é‡è¯•1æ¬¡ï¼‰
        log_info(f"ğŸ“¤ ä¸Šä¼ CSVï¼ˆç¬¬{attempt}æ¬¡å°è¯•ï¼‰: {os.path.basename(csv_file)}")
        if _do_upload(csv_file):
            return True
        if attempt < 2:
            log_warning("ä¸Šä¼ å¤±è´¥ï¼Œ5ç§’åé‡è¯•...")
            time.sleep(5)
    return False


def _do_upload(csv_file: str) -> bool:
    """æ‰§è¡Œä¸€æ¬¡ä¸Šä¼ """
    # 1. ä¸‹è½½Cookie
    cookie_list = download_cookies()
    if not cookie_list:
        return False

    from requests.cookies import RequestsCookieJar
    jar = RequestsCookieJar()
    for c in cookie_list:
        domain = c.get('domain', '')
        path   = c.get('path', '/')
        jar.set(c['name'], c['value'], domain=domain, path=path)

    session = requests.Session()
    session.cookies = jar

    cookies_dict     = {c['name']: c['value'] for c in cookie_list}
    session_token    = cookies_dict.get('_shopify_s', '')
    multitrack_token = cookies_dict.get('_shopify_y', '')

    # 2. è·å–æ–‡ä»¶ä¿¡æ¯
    file_path = Path(csv_file)
    file_size = file_path.stat().st_size
    filename  = file_path.name
    log_info(f"æ–‡ä»¶: {filename}ï¼Œå¤§å°: {file_size} bytes")

    # 3. è·å–CSRF Tokenï¼ˆSelenium çœŸå®æµè§ˆå™¨ï¼Œç»•è¿‡ Shopify Bot æ£€æµ‹ï¼‰
    csrf_token = _get_csrf_token_selenium(cookie_list)
    if not csrf_token:
        return False

    # 4. è·å–ä¸Šä¼ å‡­è¯
    log_info("è·å–GCSä¸Šä¼ å‡­è¯...")
    api_url = (f"https://admin.shopify.com/api/operations/"
               f"a2199f150c46ccdff0a4ea14b2362f7b6c06412eee6d360d8f0e128486e39cf4/"
               f"ProductCSVStageUploads/shopify/{STORE_ID}")

    req_headers = {
        'accept': 'application/json',
        'accept-language': 'zh-CN,zh;q=0.9',
        'apollographql-client-name': 'core',
        'cache-control': 'no-cache,no-store,must-revalidate,max-age=0',
        'content-type': 'application/json',
        'origin': 'https://admin.shopify.com',
        'referer': f'https://admin.shopify.com/store/{STORE_ID}/products?selectedView=all',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                      '(KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36',
        'shopify-proxy-api-enable': 'true',
        'target-manifest-route-id': 'products:list',
        'target-pathname': '/store/:storeHandle/products',
        'target-slice': 'products-section',
        'x-csrf-token': csrf_token,
    }

    page_view_token = str(uuid.uuid4())
    payload = {
        "operationName": "ProductCSVStageUploads",
        "variables": {
            "input": [{
                "filename": filename,
                "mimeType": "text/csv",
                "httpMethod": "POST",
                "fileSize": str(file_size),
                "resource": "PRODUCT_IMPORT"
            }]
        },
        "extensions": {
            "client_context": {
                "page_view_token": page_view_token,
                "client_route_handle": "products:list",
                "client_pathname": f"/store/{STORE_ID}/products",
                "client_normalized_pathname": "/store/:storeHandle/products",
                "shopify_session_token": session_token,
                "shopify_multitrack_token": multitrack_token
            }
        }
    }

    try:
        resp = session.post(api_url, headers=req_headers, json=payload, timeout=30)
        if resp.status_code != 200:
            log_error(f"è·å–å‡­è¯å¤±è´¥: {resp.status_code} {resp.text[:300]}")
            return False

        result = resp.json()
        if 'errors' in result:
            for err in result['errors']:
                log_error(f"GraphQLé”™è¯¯: {err.get('message', '')}")
            return False

        staged = result['data']['stagedUploadsCreate']['stagedTargets'][0]
        upload_url = staged['url']
        parameters  = staged['parameters']
        log_info(f"âœ… è·å–ä¸Šä¼ å‡­è¯æˆåŠŸ")
    except Exception as e:
        log_error(f"è·å–å‡­è¯å¼‚å¸¸: {e}")
        return False

    # 5. ä¸Šä¼ åˆ°GCS
    log_info("ä¸Šä¼ æ–‡ä»¶åˆ°Google Cloud Storage...")
    try:
        files_data = {}
        for param in parameters:
            files_data[param['name']] = (None, param['value'])

        with open(csv_file, 'rb') as f:
            files_data['file'] = (filename, f, 'text/csv')
            upload_headers = {
                'accept': '*/*',
                'origin': 'https://admin.shopify.com',
                'referer': 'https://admin.shopify.com/',
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            up_resp = requests.post(upload_url, headers=upload_headers,
                                    files=files_data, timeout=60)

        if up_resp.status_code in [200, 201, 204]:
            log_info("âœ… CSVä¸Šä¼ åˆ°GCSæˆåŠŸï¼")
            return True
        else:
            log_error(f"GCSä¸Šä¼ å¤±è´¥: {up_resp.status_code} {up_resp.text[:300]}")
            return False
    except Exception as e:
        log_error(f"GCSä¸Šä¼ å¼‚å¸¸: {e}")
        return False


# ============================================================
# ä¸»å¾ªç¯
# ============================================================

def process_one_task(analyzer: ZhipuImageAnalyzer) -> str:
    """
    å¤„ç†å•æ¡ä»»åŠ¡
    è¿”å›å€¼: 'success' / 'failed' / 'skipped'
    """
    # 1. è·å–ä»»åŠ¡
    task = fetch_one_task()
    if not task:
        log_info("æš‚æ— å¾…å¤„ç†ä»»åŠ¡ï¼Œç­‰å¾…ä¸‹ä¸€è½®...")
        return 'skipped'

    keer_product_id  = task.get('keer_product_id')
    client_product_url   = task.get('client_product_url')
    client_product_image = task.get('client_product_image')
    quotation_result     = task.get('quotation_result')

    log_info(f"--- å¼€å§‹å¤„ç†ä»»åŠ¡: {keer_product_id} ---")
    log_info(f"å•†å“URL: {client_product_url}")

    # 2. è§£æä»·æ ¼
    price = parse_price_from_quotation(quotation_result)
    if price is None:
        log_warning("ä»·æ ¼è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä»·æ ¼ 0.0")
        price = 0.0
    log_info(f"è§£æä»·æ ¼: ${price}")

    # 3. æŠ“å–å•†å“
    scraper = ShopifyScraper()
    product = scraper.fetch(client_product_url)
    if not product:
        log_error("å•†å“æŠ“å–å¤±è´¥")
        feedback_task_status(keer_product_id, 2)
        return 'failed'

    log_info(f"å•†å“æ ‡é¢˜: {product.title} | å˜ä½“: {len(product.variants)} | å›¾ç‰‡: {len(product.images)}")

    # 4. AIåˆ†ç±»
    category = None
    if client_product_image:
        log_info("æ­£åœ¨è¯†åˆ«å•†å“åˆ†ç±»...")
        category = get_product_category(analyzer, client_product_image)
    log_info(f"å•†å“åˆ†ç±»: {category or 'æœªè®¾ç½®'}")

    # 5. ç”ŸæˆCSVï¼ˆä¿å­˜åˆ°C:\ShopifyAutoLog\csv\ï¼‰
    csv_dir = os.path.join(LOG_DIR, 'csv')
    os.makedirs(csv_dir, exist_ok=True)
    csv_path = os.path.join(csv_dir, f"shopify_import_{keer_product_id}.csv")

    if not generate_shopify_csv(product, price, category, csv_path):
        log_error("CSVç”Ÿæˆå¤±è´¥")
        feedback_task_status(keer_product_id, 2)
        return 'failed'

    # 6. ä¸Šä¼ CSV
    upload_ok = upload_csv_to_shopify(csv_path)

    if upload_ok:
        feedback_task_status(keer_product_id, 1)
        log_info(f"âœ… ä»»åŠ¡å®Œæˆ: {keer_product_id}")
        return 'success'
    else:
        feedback_task_status(keer_product_id, 2)
        log_error(f"âŒ ä»»åŠ¡å¤±è´¥: {keer_product_id}")
        return 'failed'


def main_loop():
    """24å°æ—¶æ— é™å¾ªç¯ä¸»å…¥å£"""
    print("\n" + "=" * 60)
    print("ğŸš€ Shopify è‡ªåŠ¨åŒ–å·¥å…· â€” æ— é™å¾ªç¯æ¨¡å¼")
    print(f"   æ¯ {TASK_INTERVAL_SECONDS // 60} åˆ†é’Ÿå¤„ç†ä¸€ä¸ªäº§å“")
    print(f"   æ—¥å¿—ç›®å½•: {LOG_DIR}")
    print("=" * 60 + "\n")

    _ensure_log_dir()
    analyzer = ZhipuImageAnalyzer()
    init_global_api_keys()

    while True:
        loop_start = time.time()
        keer_product_id = None

        try:
            # å…ˆå·çœ‹ä¸€ä¸‹ä»»åŠ¡IDï¼Œç”¨äºæ—¥å¿—
            task_preview = fetch_one_task()
            keer_product_id = task_preview.get('keer_product_id') if task_preview else None

            result = process_one_task(analyzer)

            if result == 'skipped':
                write_daily_log('-', 'skipped', 'æ— å¾…å¤„ç†ä»»åŠ¡')
                write_daily_summary()
                log_info(f"ç­‰å¾… {NO_TASK_WAIT_SECONDS} ç§’åé‡è¯•...")
                time.sleep(NO_TASK_WAIT_SECONDS)
                continue  # è·³è¿‡3åˆ†é’Ÿç­‰å¾…ï¼Œç›´æ¥å†è½®è¯¢

            elif result == 'success':
                write_daily_log(keer_product_id or '-', 'success', 'ç”Ÿæˆ+ä¸Šä¼ å‡æˆåŠŸ')

            elif result == 'failed':
                write_daily_log(keer_product_id or '-', 'failed', 'ç”Ÿæˆæˆ–ä¸Šä¼ å¤±è´¥')

            write_daily_summary()

        except Exception as e:
            log_error(f"ğŸ”´ ä¸»å¾ªç¯å¼‚å¸¸ï¼ˆä¸ä¸­æ–­ç¨‹åºï¼‰: {e}")
            import traceback
            traceback.print_exc()
            write_daily_log(keer_product_id or '-', 'failed', f'ä¸»å¾ªç¯å¼‚å¸¸: {str(e)[:100]}')
            write_daily_summary()

        # ç­‰å¾…è‡³æ»¡3åˆ†é’Ÿ
        elapsed = time.time() - loop_start
        remaining = TASK_INTERVAL_SECONDS - elapsed
        if remaining > 0:
            log_info(f"â±ï¸ æœ¬æ¬¡è€—æ—¶ {elapsed:.1f}sï¼Œç­‰å¾… {remaining:.1f}s åå¤„ç†ä¸‹ä¸€ä¸ª...")
            time.sleep(remaining)


# ============================================================
# å½±åˆ€ RPA å…¥å£ï¼ˆä¾›å½±åˆ€ç›´æ¥è°ƒç”¨ï¼‰
# ============================================================

def shopify_run(args=None):
    """
    å½±åˆ€ RPA ç»Ÿä¸€å…¥å£å‡½æ•°ã€‚
    åœ¨å½±åˆ€ä¸­é…ç½®ã€Œæ‰§è¡ŒPythonå‡½æ•°ã€ï¼Œå‡½æ•°åå¡« shopify_runï¼Œå³å¯å…¨è‡ªåŠ¨è¿è¡Œã€‚
    args å‚æ•°ç”±å½±åˆ€å¹³å°ä¼ å…¥ï¼Œå¯å¿½ç•¥ã€‚
    """
    main_loop()


# ============================================================
# ç¨‹åºå…¥å£
# ============================================================

if __name__ == "__main__":
    main_loop()
