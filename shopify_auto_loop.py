# -*- coding: utf-8 -*-
"""
Shopify CSV è‡ªåŠ¨ç”Ÿæˆ + ä¸Šä¼ 
æ”¯æŒå•æ¬¡æµ‹è¯• (process_one_task) å’Œæ— é™å¾ªç¯æ¨¡å¼ (run_forever)ã€‚
"""

import csv
import json
import os
import re
import time
import traceback
import threading
import uuid
import requests
import pymysql
from selenium import webdriver
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.chrome.service import Service as ChromeService

from datetime import datetime
from typing import Optional, List, Dict
from dataclasses import dataclass, field
from urllib import parse
from pathlib import Path


# ============================================================
# å…¨å±€é…ç½®
# ============================================================

# ç¦ç”¨ç³»ç»Ÿä»£ç†
os.environ['NO_PROXY'] = '*'
os.environ['no_proxy'] = '*'
for _k in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
    os.environ.pop(_k, None)

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    "host": "47.95.157.46",
    "user": "root",
    "password": "root@kunkun",
    "port": 3306,
    "database": "quote_iw",
    "charset": "utf8mb4"
}

# APIåŸºç¡€åœ°å€
API_BASE_URL     = "http://47.95.157.46:8520"
LOG_API_BASE_URL = "http://47.104.72.198:2580"

# Shopifyé…ç½®
STORE_ID   = "893848-2"
COOKIE_URL = "https://ceshi-1300392622.cos.ap-beijing.myqcloud.com/shopify-cookies/893848-2.json"

# åº“å­˜åŒæ­¥é…ç½®
INVENTORY_LOCATION_ID   = "83358875936"
INVENTORY_LOCATION_NAME = "ç‰Ÿå¹³åŒºåŒ—å…³å¤§è¡—845"
AUTODS_LOCATION_NAME    = "AutoDS prod-pfhikdgf"   # AutoDS ä»“åº“ä½ç½®ï¼ˆå›ºå®šï¼‰
INVENTORY_WAIT_SECONDS  = 120              # äº§å“å¯¼å…¥åç­‰å¾…ç§’æ•°ï¼ˆ1-2åˆ†é’Ÿï¼‰
INVENTORY_QUANTITY      = 100              # å›ºå®šåº“å­˜æ•°é‡

# æ—¥å¿—ç›®å½•
LOG_DIR = r"C:\ShopifyAutoLog"


# ============================================================
# æ—¥å¿—å‡½æ•°
# ============================================================

def log_info(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] INFO: {msg}")

def log_warning(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] WARNING: {msg}")

def log_error(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ERROR: {msg}")


# ============================================================
# æ¯æ—¥ç»Ÿè®¡æ—¥å¿—
# ============================================================

def _ensure_log_dir():
    os.makedirs(LOG_DIR, exist_ok=True)

def _today_log_path() -> str:
    _ensure_log_dir()
    date_str = datetime.now().strftime('%Y-%m-%d')
    return os.path.join(LOG_DIR, f"shopify_{date_str}.log")

def write_daily_log(keer_product_id: str, result: str, detail: str = ""):
    log_path = _today_log_path()
    now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    line = f"[{now_str}] [{result.upper():8s}] ID={str(keer_product_id or '-'):30s} {detail}\n"
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(line)
    _write_db_log(keer_product_id, result, detail)


def _write_db_log(keer_product_id: str, result: str, detail: str = ""):
    try:
        conn = pymysql.connect(**DB_CONFIG)
        try:
            with conn.cursor() as cursor:
                sql = """
                    INSERT INTO shopify_task_log
                        (task_date, keer_product_id, result, detail, created_at)
                    VALUES (%s, %s, %s, %s, %s)
                """
                now = datetime.now()
                cursor.execute(sql, (
                    now.strftime('%Y-%m-%d'),
                    keer_product_id or '',
                    result,
                    detail[:500] if detail else '',
                    now.strftime('%Y-%m-%d %H:%M:%S')
                ))
            conn.commit()
        finally:
            conn.close()
    except Exception as e:
        log_error(f"DBæ—¥å¿—å†™å…¥å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")


# ============================================================
# æ•°æ®ç±»
# ============================================================

@dataclass
class ProductVariant:
    id: int = 0
    title: str = ""
    price: str = "0"
    compare_at_price: Optional[str] = None
    sku: str = ""
    available: bool = True
    option1: Optional[str] = None
    option2: Optional[str] = None
    option3: Optional[str] = None
    grams: int = 0

@dataclass
class ProductImage:
    id: int = 0
    src: str = ""
    alt: Optional[str] = None
    position: int = 0

@dataclass
class ProductDetail:
    id: int = 0
    title: str = ""
    handle: str = ""
    description: str = ""
    vendor: str = ""
    product_type: str = ""
    tags: List[str] = field(default_factory=list)
    variants: List[ProductVariant] = field(default_factory=list)
    images: List[ProductImage] = field(default_factory=list)
    options: List[Dict] = field(default_factory=list)


# ============================================================
# Shopifyå•†å“æŠ“å–
# ============================================================

class ShopifyScraper:
    def __init__(self, timeout: int = 15):
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json'
        })
        self.session.trust_env = False

    def fetch(self, product_url: str) -> Optional[ProductDetail]:
        is_preview = 'preview_key=' in product_url or 'products_preview' in product_url
        if is_preview:
            if '?' in product_url:
                base_url = product_url.split('?')[0]
                params = product_url.split('?')[1]
                if not base_url.endswith('.json'):
                    base_url = base_url.rstrip('/') + '.json'
                json_url = f"{base_url}?{params}"
            else:
                json_url = product_url.rstrip('/') + '.json'
        else:
            json_url = product_url.split('?')[0].rstrip('/')
            if not json_url.endswith('.json'):
                json_url += '.json'

        try:
            log_info(f"æ­£åœ¨è®¿é—®: {json_url}")
            response = self.session.get(json_url, timeout=self.timeout)
            if response.status_code == 404:
                log_error(f"å•†å“ä¸å­˜åœ¨(404): {product_url}")
                return None
            response.raise_for_status()
            data = response.json()
            if 'product' not in data:
                log_error("å“åº”ä¸­æ²¡æœ‰productå­—æ®µ")
                return None
            return self._parse(data['product'])
        except Exception as e:
            log_error(f"æŠ“å–é”™è¯¯: {e}")
            return None

    def _parse(self, data: Dict) -> ProductDetail:
        variants = []
        for v in data.get('variants', []):
            variants.append(ProductVariant(
                id=v.get('id', 0), title=v.get('title', ''),
                price=v.get('price', '0'), compare_at_price=v.get('compare_at_price'),
                sku=v.get('sku', ''), available=v.get('available', True),
                option1=v.get('option1'), option2=v.get('option2'),
                option3=v.get('option3'), grams=v.get('grams', 0)
            ))
        images = []
        for img in data.get('images', []):
            images.append(ProductImage(
                id=img.get('id', 0), src=img.get('src', ''),
                alt=img.get('alt'), position=img.get('position', 0)
            ))
        tags = data.get('tags', '')
        if isinstance(tags, str):
            tags = [t.strip() for t in tags.split(',') if t.strip()]
        return ProductDetail(
            id=data.get('id', 0), title=data.get('title', ''),
            handle=data.get('handle', ''), description=data.get('body_html', ''),
            vendor=data.get('vendor', ''), product_type=data.get('product_type', ''),
            tags=tags, variants=variants, images=images,
            options=data.get('options', [])
        )


# ============================================================
# ZhipuAI APIå¯†é’¥ç®¡ç†
# ============================================================

_global_zhipuai_keys = []
_keys_cache_lock = threading.Lock()


def _fetch_api_keys_with_retry(api_url: str, max_retries: int = 3) -> list:
    headers = {'Content-Type': 'application/x-www-form-urlencoded'}
    data = parse.urlencode({}, True)
    proxies = {'http': None, 'https': None}
    for attempt in range(1, max_retries + 1):
        try:
            log_info(f"ğŸ“¡ æ­£åœ¨è·å–ZhipuAIå¯†é’¥ (ç¬¬{attempt}/{max_retries}æ¬¡)...")
            response = requests.post(api_url, headers=headers, data=data, timeout=10, proxies=proxies)
            if response.status_code == 200:
                result = response.json()
                if result.get("success") and "data" in result:
                    keys = [item["key"].strip() for item in result["data"]]
                    log_info(f"âœ… æˆåŠŸè·å– {len(keys)} ä¸ªZhipuAI APIå¯†é’¥")
                    return keys
        except requests.exceptions.Timeout:
            log_warning(f"âš ï¸ å¯†é’¥è·å–è¶…æ—¶ (ç¬¬{attempt}/{max_retries}æ¬¡)")
            if attempt < max_retries: time.sleep(2 ** attempt)
        except Exception as e:
            log_error(f"âŒ å¯†é’¥è·å–å¼‚å¸¸: {e}")
            if attempt < max_retries: time.sleep(2 ** attempt)
    return []


def init_global_api_keys() -> bool:
    global _global_zhipuai_keys
    _global_zhipuai_keys = _fetch_api_keys_with_retry(f'{API_BASE_URL}/api/zhipuai_key', max_retries=3)
    log_info(f"ğŸ”‘ ZhipuAIå¯†é’¥ç¼“å­˜åˆå§‹åŒ–å®Œæˆ ({len(_global_zhipuai_keys)} ä¸ª)")
    return len(_global_zhipuai_keys) > 0


def get_cached_zhipuai_keys() -> list:
    with _keys_cache_lock:
        return _global_zhipuai_keys.copy()


def refresh_api_keys() -> bool:
    global _global_zhipuai_keys
    with _keys_cache_lock:
        keys = _fetch_api_keys_with_retry(f'{API_BASE_URL}/api/zhipuai_key', max_retries=3)
        if keys:
            _global_zhipuai_keys = keys
            return True
        return False


class APIKeyManager:
    def __init__(self, blacklist_duration=180):
        self.blacklist_duration = blacklist_duration
        self.blacklisted_keys = {}
        self.lock = threading.RLock()
        self.last_used_index = -1

    def add_to_blacklist(self, api_key: str, reason: str = "è°ƒç”¨å¤±è´¥"):
        with self.lock:
            self.blacklisted_keys[api_key] = time.time()
            log_warning(f"âš ï¸ å¯†é’¥åŠ å…¥é»‘åå•({reason}): ...{api_key[-8:]} ({self.blacklist_duration}ç§’)")

    def is_blacklisted(self, api_key: str) -> bool:
        with self.lock:
            if api_key not in self.blacklisted_keys:
                return False
            if time.time() - self.blacklisted_keys[api_key] >= self.blacklist_duration:
                del self.blacklisted_keys[api_key]
                return False
            return True

    def get_next_available_key(self, all_keys: list) -> Optional[str]:
        with self.lock:
            if not all_keys:
                return None
            expired = [k for k, t in self.blacklisted_keys.items()
                       if time.time() - t >= self.blacklist_duration]
            for k in expired:
                del self.blacklisted_keys[k]
            available = [k for k in all_keys if not self.is_blacklisted(k)]
            if not available:
                log_error("âŒ æ‰€æœ‰å¯†é’¥éƒ½åœ¨é»‘åå•ä¸­ï¼")
                return None
            self.last_used_index = (self.last_used_index + 1) % len(available)
            selected = available[self.last_used_index]
            log_info(f"ğŸ”‘ è½®æ¢å¯†é’¥[{self.last_used_index + 1}/{len(available)}]: ...{selected[-8:]}")
            return selected

    def record_success(self, api_key: str):
        pass

    def record_failure(self, api_key: str, error_msg: str = ""):
        if any(kw in error_msg.lower() for kw in ['rate limit', 'too many', '429', 'å¹¶å‘', 'é™æµ']):
            self.add_to_blacklist(api_key, "é™æµ")


api_key_manager = APIKeyManager(blacklist_duration=180)


def zhipu_single_image_analyze_sync(image_url: str, prompt: str, max_retries: int = 10) -> str:
    wait_all_blocked = 30
    for attempt in range(max_retries):
        selected_key = None
        try:
            api_keys = get_cached_zhipuai_keys()
            if not api_keys:
                if refresh_api_keys():
                    api_keys = get_cached_zhipuai_keys()
            if not api_keys:
                raise Exception("æ— æ³•è·å–ZhipuAI APIå¯†é’¥")

            selected_key = api_key_manager.get_next_available_key(api_keys)
            if not selected_key:
                log_warning(f"âš ï¸ æ‰€æœ‰å¯†é’¥åœ¨é»‘åå•ï¼Œç­‰å¾…{wait_all_blocked}ç§’...")
                time.sleep(wait_all_blocked)
                selected_key = api_key_manager.get_next_available_key(api_keys)
                if not selected_key:
                    continue

            url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
            headers = {"Authorization": f"Bearer {selected_key}", "Content-Type": "application/json"}
            payload = {
                "model": "glm-4.1v-thinking-flash",
                "max_tokens": 16384,
                "top_p": 0.1,
                "messages": [{"role": "user", "content": [
                    {"type": "image_url", "image_url": {"url": image_url}},
                    {"type": "text", "text": prompt}
                ]}]
            }
            response = requests.post(url, headers=headers, json=payload, timeout=120)
            if response.status_code == 200:
                result = response.json()
                if result and 'choices' in result and result['choices']:
                    content = result['choices'][0].get('message', {}).get('content', '')
                    if content:
                        api_key_manager.record_success(selected_key)
                        return content
                raise Exception(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}, {response.text}")
        except Exception as e:
            error_msg = str(e)
            log_error(f"âŒ ZhipuAIåˆ†æå¤±è´¥(ç¬¬{attempt+1}æ¬¡): {error_msg}")
            if selected_key:
                api_key_manager.record_failure(selected_key, error_msg)
            sleep_t = 5 if '429' in error_msg else (2 if 'timeout' in error_msg.lower() else 0.5)
            time.sleep(sleep_t)
    return "åˆ†æå¤±è´¥ï¼šè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"


class ZhipuImageAnalyzer:
    def __init__(self):
        self._initialized = False

    def _ensure_initialized(self):
        if not self._initialized:
            if not get_cached_zhipuai_keys():
                init_global_api_keys()
            self._initialized = True

    def analyze(self, image_url: str, prompt: str) -> str:
        self._ensure_initialized()
        return zhipu_single_image_analyze_sync(image_url, prompt)


# ============================================================
# AIåˆ†ç±»
# ============================================================

def get_product_category(analyzer: ZhipuImageAnalyzer, image_url: str) -> Optional[str]:
    try:
        prompt = """Identify the product in the image and return the most appropriate category from this list:

1. Apparel & Accessories (clothing, shoes, jewelry, watches, hats, scarves)
2. Luggage & Bags (handbags, backpacks, wallets, suitcases)
3. Animals & Pet Supplies (pet food, pet toys, pet accessories)
4. Home & Garden (kitchen, bedding, lighting, garden tools, home decor)
5. Furniture (tables, chairs, sofas, beds, desks)
6. Electronics (phones, computers, audio, TV, smart devices)
7. Cameras & Optics (cameras, lenses, binoculars)
8. Health & Beauty (skincare, makeup, hair care, personal care)
9. Sporting Goods (fitness, outdoor, cycling, sports equipment)
10. Toys & Games (toys, games, puzzles)
11. Baby & Toddler (baby products, strollers, baby clothing)
12. Office Supplies (stationery, office equipment)
13. Vehicles & Parts (car accessories, auto parts)
14. Food, Beverages & Tobacco (food, drinks, snacks)
15. Hardware (tools, building materials)
16. Arts & Entertainment (art supplies, musical instruments)
17. Media (books, music, movies)

Rules:
1. Return ONLY the category name exactly as shown (e.g., "Apparel & Accessories")
2. Choose the single best matching category
3. No explanation, no punctuation, just the category name"""

        result = analyzer.analyze(image_url, prompt)
        if result and 'åˆ†æå¤±è´¥' not in result:
            category = result.strip()
            category = re.sub(r'<\|[^|]+\|>', '', category)
            category = re.sub(r'<think>.*?</think>', '', category, flags=re.IGNORECASE | re.DOTALL)
            category = re.sub(r'<[^>]+>', '', category)
            category = category.strip().split('\n')[0].strip()
            if len(category) > 200:
                category = category[:200]
            log_info(f"ğŸ·ï¸ AIåˆ†ç±»ç»“æœ: {category}")
            return category if category else None
        return None
    except Exception as e:
        log_error(f"âŒ AIåˆ†ç±»å¼‚å¸¸: {e}")
        return None


# ============================================================
# æ•°æ®åº“æ“ä½œ
# ============================================================

def fetch_one_task() -> Optional[Dict]:
    conn = pymysql.connect(**DB_CONFIG)
    try:
        with conn.cursor(pymysql.cursors.DictCursor) as cursor:
            sql = """
                SELECT keer_product_id, client_product_url, client_product_image,
                       quotation_result, created_at
                FROM quotation_task_detail
                WHERE created_at >= DATE_SUB(NOW(), INTERVAL 3 DAY)
                  AND task_status = 'æŠ¥ä»·å•åˆ›å»ºå®Œæ¯•'
                  AND client_product_url LIKE 'http%'
                  AND (shopfiy_task IS NULL OR shopfiy_task = '')
                  AND client_product_image IS NOT NULL
                  AND client_product_image != ''
                ORDER BY created_at DESC
                LIMIT 1
            """
            cursor.execute(sql)
            return cursor.fetchone()
    finally:
        conn.close()


def parse_price_from_quotation(quotation_result: str) -> Optional[float]:
    try:
        if not quotation_result:
            return None
        data = json.loads(quotation_result)
        if not isinstance(data, list) or not data:
            return None
        qty1_items = [item for item in data if item.get('quantity') == 1]
        if not qty1_items:
            return None
        for item in qty1_items:
            if item.get('nation') == 'US':
                return float(item.get('price', 0))
        return float(qty1_items[0].get('price', 0))
    except Exception as e:
        log_error(f"ä»·æ ¼è§£æé”™è¯¯: {e}")
        return None


def feedback_task_status(keer_product_id: str, shopfiy_task: int) -> bool:
    try:
        url = f'{API_BASE_URL}/api/task-data/save'
        payload = {"keer_product_id": keer_product_id, "shopfiy_task": shopfiy_task}
        response = requests.post(url, headers={'Content-Type': 'application/json'},
                                 json=payload, timeout=10)
        if response.status_code == 200:
            log_info(f"âœ… çŠ¶æ€åé¦ˆ: {keer_product_id} -> {'æˆåŠŸ' if shopfiy_task == 1 else 'å¤±è´¥'}")
            return True
        log_error(f"çŠ¶æ€åé¦ˆå¤±è´¥: {response.status_code}")
        return False
    except Exception as e:
        log_error(f"çŠ¶æ€åé¦ˆå¼‚å¸¸: {e}")
        return False


# ============================================================
# CSVç”Ÿæˆï¼ˆShopify 2024æ ¼å¼ï¼‰
# ============================================================

def generate_shopify_csv(product: ProductDetail, price: float, category: str,
                         output_path: str) -> bool:
    headers = [
        'Title', 'URL handle', 'Description', 'Vendor', 'Product category', 'Type', 'Tags',
        'Published on online store', 'Status',
        'SKU', 'Barcode',
        'Option1 name', 'Option1 value', 'Option1 Linked To',
        'Option2 name', 'Option2 value', 'Option2 Linked To',
        'Option3 name', 'Option3 value', 'Option3 Linked To',
        'Price', 'Compare-at price', 'Cost per item',
        'Charge tax', 'Tax code',
        'Inventory tracker', 'Inventory quantity', 'Continue selling when out of stock',
        'Weight value (grams)', 'Weight unit for display', 'Requires shipping', 'Fulfillment service',
        'Product image URL', 'Image position', 'Image alt text', 'Variant image URL',
        'Gift card', 'SEO title', 'SEO description'
    ]
    rows = []
    handle = product.handle or re.sub(r'[^a-z0-9]+', '-', product.title.lower()).strip('-')
    safe_category = (category or '')[:200]
    option1_name = product.options[0].get('name', 'Title') if product.options else 'Title'
    option2_name = product.options[1].get('name', '') if len(product.options) > 1 else ''
    option3_name = product.options[2].get('name', '') if len(product.options) > 2 else ''

    for i, variant in enumerate(product.variants):
        row = {
            'Title': product.title if i == 0 else '',
            'URL handle': handle,
            'Description': product.description if i == 0 else '',
            'Vendor': product.vendor if i == 0 else '',
            'Product category': safe_category if i == 0 else '',
            'Type': safe_category if i == 0 else '',
            'Tags': ', '.join(product.tags) if i == 0 else '',
            'Published on online store': 'TRUE' if i == 0 else '',
            'Status': 'active' if i == 0 else '',
            'SKU': variant.sku or '',
            'Barcode': '',
            'Option1 name': option1_name if i == 0 else '',
            'Option1 value': variant.option1 or 'Default Title',
            'Option1 Linked To': '',
            'Option2 name': option2_name if i == 0 else '',
            'Option2 value': variant.option2 or '',
            'Option2 Linked To': '',
            'Option3 name': option3_name if i == 0 else '',
            'Option3 value': variant.option3 or '',
            'Option3 Linked To': '',
            'Price': str(price),
            'Compare-at price': variant.compare_at_price or '',
            'Cost per item': '',
            'Charge tax': 'TRUE',
            'Tax code': '',
            'Inventory tracker': 'shopify',
            'Inventory quantity': '100',
            'Continue selling when out of stock': 'DENY',
            'Weight value (grams)': str(variant.grams) if variant.grams else '0',
            'Weight unit for display': 'g',
            'Requires shipping': 'TRUE',
            'Fulfillment service': 'manual',
            'Product image URL': product.images[0].src if i == 0 and product.images else '',
            'Image position': '1' if i == 0 and product.images else '',
            'Image alt text': product.images[0].alt or '' if i == 0 and product.images else '',
            'Variant image URL': '',
            'Gift card': 'FALSE' if i == 0 else '',
            'SEO title': product.title[:70] if i == 0 else '',
            'SEO description': ''
        }
        rows.append(row)

    for img_idx, img in enumerate(product.images[1:], start=2):
        row = {h: '' for h in headers}
        row['URL handle'] = handle
        row['Product image URL'] = img.src
        row['Image position'] = str(img_idx)
        row['Image alt text'] = img.alt or ''
        rows.append(row)

    try:
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            writer.writerows(rows)
        log_info(f"CSVæ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
        return True
    except Exception as e:
        log_error(f"CSVå†™å…¥å¤±è´¥: {e}")
        return False


# ============================================================
# Cookie çŠ¶æ€ä¸ŠæŠ¥
# ============================================================

def _report_cookie_status_worker(is_valid: bool, detail: str):
    try:
        url = f"{LOG_API_BASE_URL}/api/shopify/cookie-status/report"
        payload = {
            "store_id": STORE_ID,
            "is_valid": is_valid,
            "checker":  "auto_loop",
            "detail":   detail[:500] if detail else "",
        }
        resp = requests.post(url, json=payload, timeout=2)
        if resp.status_code == 200:
            status_str = "æœ‰æ•ˆ" if is_valid else "å¤±æ•ˆ"
            log_info(f"CookieçŠ¶æ€å·²ä¸ŠæŠ¥: {status_str} | {detail}")
        else:
            log_warning(f"CookieçŠ¶æ€ä¸ŠæŠ¥å¤±è´¥: HTTP {resp.status_code}")
    except Exception as e:
        log_warning(f"CookieçŠ¶æ€ä¸ŠæŠ¥å¼‚å¸¸ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")


def report_cookie_status(is_valid: bool, detail: str = ""):
    t = threading.Thread(target=_report_cookie_status_worker, args=(is_valid, detail), daemon=True)
    t.start()


# ============================================================
# Cookieä¸‹è½½
# ============================================================

def download_cookies() -> Optional[list]:
    try:
        log_info(f"æ­£åœ¨ä¸‹è½½Cookie: {COOKIE_URL}")
        resp = requests.get(COOKIE_URL, timeout=15)
        resp.raise_for_status()
        data = resp.json()

        cookie_list = []
        if isinstance(data, dict) and 'cookies' in data:
            for c in data['cookies']:
                if 'name' in c and 'value' in c:
                    cookie_list.append(c)
        elif isinstance(data, list):
            for c in data:
                if 'name' in c and 'value' in c:
                    cookie_list.append(c)

        log_info(f"âœ… CookieåŠ è½½æˆåŠŸï¼Œå…± {len(cookie_list)} ä¸ª")
        return cookie_list
    except Exception as e:
        log_error(f"âŒ Cookieä¸‹è½½å¤±è´¥: {e}")
        return None


# ============================================================
# Shopify CSVä¸Šä¼ 
# ============================================================

def _get_csrf_token_selenium(cookie_list: list) -> Optional[str]:
    url = f"https://admin.shopify.com/store/{STORE_ID}/products?selectedView=all"
    driver = None
    try:
        chrome_options = ChromeOptions()
        chrome_options.add_argument('--headless=new')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--window-size=1280,800')
        chrome_options.add_argument('--lang=zh-CN')
        chrome_options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
            '(KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'
        )
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)

        driver = webdriver.Chrome(options=chrome_options)
        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
            'source': "Object.defineProperty(navigator, 'webdriver', {get: () => undefined});"
        })

        log_info("ğŸŒ Selenium æ­£åœ¨åŠ è½½ Shopify åå°...")
        driver.get("https://admin.shopify.com/")
        time.sleep(1)

        for c in cookie_list:
            cookie_entry = {
                'name':   c['name'],
                'value':  c['value'],
                'domain': c.get('domain', '.shopify.com'),
                'path':   c.get('path', '/'),
            }
            if c.get('secure'):
                cookie_entry['secure'] = True
            if c.get('httpOnly'):
                cookie_entry['httpOnly'] = True
            try:
                driver.add_cookie(cookie_entry)
            except Exception:
                pass

        driver.get(url)
        time.sleep(5)

        content = driver.page_source
        pattern = r'<script type="text/json" data-serialized-id="server-data">\s*(\{.*?\})\s*</script>'
        match = re.search(pattern, content, re.DOTALL)
        if not match:
            current_url = driver.current_url
            log_error(f"æœªæ‰¾åˆ° server-dataï¼Œå½“å‰URL: {current_url}")
            if 'login' in current_url or 'accounts.shopify.com' in current_url:
                report_cookie_status(False, "è¢«é‡å®šå‘è‡³ç™»å½•é¡µï¼ŒCookie å·²å¤±æ•ˆ")
            else:
                report_cookie_status(False, "Selenium æœªæ‰¾åˆ° server-dataï¼ŒCookie å¯èƒ½å·²å¤±æ•ˆ")
            return None

        server_data = json.loads(match.group(1))
        token = server_data.get('csrfToken')
        if token:
            log_info(f"âœ… Selenium è·å– CSRF token æˆåŠŸ: {token[:30]}...")
            report_cookie_status(True, "Selenium CSRF token è·å–æˆåŠŸï¼ŒCookie æœ‰æ•ˆ")
            return token

        log_error("server-data ä¸­æ—  csrfToken å­—æ®µ")
        report_cookie_status(False, "server-data ä¸­æ—  csrfToken å­—æ®µï¼ŒCookie å¯èƒ½å·²å¤±æ•ˆ")
        return None

    except Exception as e:
        log_error(f"Selenium è·å– CSRF token å¼‚å¸¸: {e}")
        report_cookie_status(False, f"Selenium å¼‚å¸¸: {str(e)[:200]}")
        return None
    finally:
        if driver:
            try:
                driver.quit()
            except Exception:
                pass


def upload_csv_to_shopify(csv_file: str) -> bool:
    for attempt in range(1, 3):
        log_info(f"ğŸ“¤ ä¸Šä¼ CSVï¼ˆç¬¬{attempt}æ¬¡å°è¯•ï¼‰: {os.path.basename(csv_file)}")
        if _do_upload(csv_file):
            return True
        if attempt < 2:
            log_warning("ä¸Šä¼ å¤±è´¥ï¼Œ5ç§’åé‡è¯•...")
            time.sleep(5)
    return False


def _do_upload(csv_file: str) -> bool:
    cookie_list = download_cookies()
    if not cookie_list:
        return False

    from requests.cookies import RequestsCookieJar
    jar = RequestsCookieJar()
    for c in cookie_list:
        domain = c.get('domain', '')
        path   = c.get('path', '/')
        jar.set(c['name'], c['value'], domain=domain, path=path)

    session = requests.Session()
    session.cookies = jar

    cookies_dict     = {c['name']: c['value'] for c in cookie_list}
    session_token    = cookies_dict.get('_shopify_s', '')
    multitrack_token = cookies_dict.get('_shopify_y', '')

    file_path = Path(csv_file)
    file_size = file_path.stat().st_size
    filename  = file_path.name
    log_info(f"æ–‡ä»¶: {filename}ï¼Œå¤§å°: {file_size} bytes")

    csrf_token = _get_csrf_token_selenium(cookie_list)
    if not csrf_token:
        return False

    log_info("è·å–GCSä¸Šä¼ å‡­è¯...")
    api_url = (f"https://admin.shopify.com/api/operations/"
               f"a2199f150c46ccdff0a4ea14b2362f7b6c06412eee6d360d8f0e128486e39cf4/"
               f"ProductCSVStageUploads/shopify/{STORE_ID}")

    req_headers = {
        'accept': 'application/json',
        'accept-language': 'zh-CN,zh;q=0.9',
        'apollographql-client-name': 'core',
        'cache-control': 'no-cache,no-store,must-revalidate,max-age=0',
        'content-type': 'application/json',
        'origin': 'https://admin.shopify.com',
        'referer': f'https://admin.shopify.com/store/{STORE_ID}/products?selectedView=all',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                      '(KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36',
        'shopify-proxy-api-enable': 'true',
        'target-manifest-route-id': 'products:list',
        'target-pathname': '/store/:storeHandle/products',
        'target-slice': 'products-section',
        'x-csrf-token': csrf_token,
    }

    page_view_token = str(uuid.uuid4())
    payload = {
        "operationName": "ProductCSVStageUploads",
        "variables": {
            "input": [{
                "filename": filename,
                "mimeType": "text/csv",
                "httpMethod": "POST",
                "fileSize": str(file_size),
                "resource": "PRODUCT_IMPORT"
            }]
        },
        "extensions": {
            "client_context": {
                "page_view_token": page_view_token,
                "client_route_handle": "products:list",
                "client_pathname": f"/store/{STORE_ID}/products",
                "client_normalized_pathname": "/store/:storeHandle/products",
                "shopify_session_token": session_token,
                "shopify_multitrack_token": multitrack_token
            }
        }
    }

    try:
        resp = session.post(api_url, headers=req_headers, json=payload, timeout=30)
        if resp.status_code != 200:
            log_error(f"è·å–å‡­è¯å¤±è´¥: {resp.status_code} {resp.text[:300]}")
            return False

        result = resp.json()
        if 'errors' in result:
            for err in result['errors']:
                log_error(f"GraphQLé”™è¯¯: {err.get('message', '')}")
            return False

        staged = result['data']['stagedUploadsCreate']['stagedTargets'][0]
        upload_url = staged['url']
        parameters  = staged['parameters']
        log_info(f"âœ… è·å–ä¸Šä¼ å‡­è¯æˆåŠŸ")
    except Exception as e:
        log_error(f"è·å–å‡­è¯å¼‚å¸¸: {e}")
        return False

    log_info("ä¸Šä¼ æ–‡ä»¶åˆ°Google Cloud Storage...")
    try:
        files_data = {}
        for param in parameters:
            files_data[param['name']] = (None, param['value'])

        with open(csv_file, 'rb') as f:
            files_data['file'] = (filename, f, 'text/csv')
            upload_headers = {
                'accept': '*/*',
                'origin': 'https://admin.shopify.com',
                'referer': 'https://admin.shopify.com/',
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            up_resp = requests.post(upload_url, headers=upload_headers,
                                    files=files_data, timeout=60)

        if up_resp.status_code in [200, 201, 204]:
            log_info("âœ… CSVä¸Šä¼ åˆ°GCSæˆåŠŸï¼")
        else:
            log_error(f"GCSä¸Šä¼ å¤±è´¥: {up_resp.status_code} {up_resp.text[:300]}")
            return False
    except Exception as e:
        log_error(f"GCSä¸Šä¼ å¼‚å¸¸: {e}")
        return False

    # æ­¥éª¤3 + æ­¥éª¤4ï¼šè§¦å‘ Shopify çœŸæ­£å¯¼å…¥
    return _trigger_shopify_import(session, req_headers, parameters,
                                   session_token, multitrack_token, page_view_token)


def _trigger_shopify_import(session: requests.Session, base_headers: dict,
                             gcs_parameters: list,
                             session_token: str, multitrack_token: str,
                             page_view_token: str) -> bool:
    """
    å®Œæ•´çš„ Shopify å¯¼å…¥æµç¨‹ï¼ˆæŠ“åŒ…ç¡®è®¤çš„çœŸå®æ¥å£ï¼‰ï¼š
      æ­¥éª¤3: ProductImportCreate  â†’ ç”¨ GCS key åˆ›å»ºå¯¼å…¥ä»»åŠ¡ï¼Œè¿”å› ProductImport ID
      æ­¥éª¤4: ProductImportSubmit  â†’ ç”¨ ID æäº¤æ‰§è¡Œï¼Œäº§å“æ‰ä¼šçœŸæ­£å‡ºç°åœ¨åå°
    """

    # ä» GCS å‚æ•°é‡Œæå– keyï¼ˆæ ¼å¼å¦‚ tmp/xxxxx/filename.csvï¼‰
    staged_key = None
    for param in gcs_parameters:
        if param.get('name') == 'key':
            staged_key = param['value']
            break

    if not staged_key:
        log_error("âŒ æœªæ‰¾åˆ° GCS staged keyï¼Œæ— æ³•è§¦å‘å¯¼å…¥")
        return False

    log_info(f"ğŸ“¥ æ­¥éª¤3: ProductImportCreateï¼Œstaged_key: {staged_key}")

    # â”€â”€ å…¬å…± headers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    common_headers = {
        'accept': 'application/json',
        'accept-language': 'zh-CN,zh;q=0.9',
        'apollographql-client-name': 'core',
        'cache-control': 'no-cache,no-store,must-revalidate,max-age=0',
        'content-type': 'application/json',
        'origin': 'https://admin.shopify.com',
        'referer': f'https://admin.shopify.com/store/{STORE_ID}/products?selectedView=all',
        'user-agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                       '(KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'),
        'shopify-proxy-api-enable': 'true',
        'target-manifest-route-id': 'products:list',
        'target-pathname': '/store/:storeHandle/products',
        'target-slice': 'products-section',
        'x-csrf-token': base_headers.get('x-csrf-token', ''),
    }

    client_context = {
        "page_view_token": page_view_token,
        "client_route_handle": "products:list",
        "client_pathname": f"/store/{STORE_ID}/products",
        "client_normalized_pathname": "/store/:storeHandle/products",
        "shopify_session_token": session_token,
        "shopify_multitrack_token": multitrack_token
    }

    # â”€â”€ æ­¥éª¤3: ProductImportCreate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    create_url = (
        f"https://admin.shopify.com/api/operations/"
        f"68c029f983cbd39de99c30c73518a1f84a1053e06c5b312ed4d994967dc36a3f/"
        f"ProductImportCreate/shopify/{STORE_ID}"
    )
    create_payload = {
        "operationName": "ProductImportCreate",
        "variables": {
            "input": {
                "url": staged_key,
                "overwrite": True,
                "publishToAllChannels": True
            }
        },
        "extensions": {"client_context": client_context}
    }

    try:
        resp = session.post(create_url, headers=common_headers, json=create_payload, timeout=30)
        log_info(f"ProductImportCreate å“åº”: HTTP {resp.status_code}")
        log_info(f"å“åº”å†…å®¹: {resp.text[:500]}")

        if resp.status_code != 200:
            log_error(f"ProductImportCreate å¤±è´¥: {resp.status_code}")
            return False

        result = resp.json()
        if 'errors' in result:
            log_error(f"ProductImportCreate GraphQL é”™è¯¯: {result['errors']}")
            return False

        # æå– ProductImport GIDï¼Œæ ¼å¼: gid://shopify/ProductImport/xxxxxxxx
        try:
            import_gid = result['data']['productImportCreate']['productImport']['id']
        except (KeyError, TypeError) as e:
            log_error(f"æ— æ³•ä»å“åº”ä¸­æå– ProductImport ID: {e}ï¼Œå“åº”: {result}")
            return False

        log_info(f"âœ… ProductImportCreate æˆåŠŸï¼ŒImport ID: {import_gid}")

    except Exception as e:
        log_error(f"ProductImportCreate å¼‚å¸¸: {e}")
        return False

    # â”€â”€ æ­¥éª¤4: ProductImportSubmit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log_info(f"ğŸ“¤ æ­¥éª¤4: ProductImportSubmitï¼ŒID: {import_gid}")

    submit_url = (
        f"https://admin.shopify.com/api/operations/"
        f"0623f4c83b0e6dfe94448cebe8295bb1ae5c3b6406ed1e9acec2d69571d477a4/"
        f"ProductImportSubmit/shopify/{STORE_ID}"
    )
    submit_payload = {
        "operationName": "ProductImportSubmit",
        "variables": {
            "id": import_gid
        },
        "extensions": {"client_context": client_context}
    }

    try:
        resp = session.post(submit_url, headers=common_headers, json=submit_payload, timeout=30)
        log_info(f"ProductImportSubmit å“åº”: HTTP {resp.status_code}")
        log_info(f"å“åº”å†…å®¹: {resp.text[:500]}")

        if resp.status_code != 200:
            log_error(f"ProductImportSubmit å¤±è´¥: {resp.status_code}")
            return False

        result = resp.json()
        if 'errors' in result:
            log_error(f"ProductImportSubmit GraphQL é”™è¯¯: {result['errors']}")
            return False

        log_info("âœ… ProductImportSubmit æˆåŠŸï¼äº§å“å°†åœ¨ Shopify åå°å¼‚æ­¥å¯¼å…¥ï¼ˆé€šå¸¸1~2åˆ†é’Ÿå†…å®Œæˆï¼‰")
        return True

    except Exception as e:
        log_error(f"ProductImportSubmit å¼‚å¸¸: {e}")
        return False


# ============================================================
# åº“å­˜åŒæ­¥ï¼ˆäº§å“å¯¼å…¥åè®¾ç½®åº“å­˜æ•°é‡ï¼‰
# ============================================================

def generate_inventory_csv(product: ProductDetail, location_name: str,
                            output_path: str, quantity: int = 100) -> bool:
    """
    ç”Ÿæˆ Shopify åº“å­˜å¯¼å…¥ CSVã€‚
    æ ¼å¼ä¸ Shopify å¯¼å‡ºçš„åº“å­˜ CSV å®Œå…¨ä¸€è‡´ï¼š
    - æ¯ä¸ªå˜ä½“ä¸¤è¡Œï¼šç‰Ÿå¹³åŒºåŒ—å…³å¤§è¡—845 è¡Œ + AutoDS è¡Œ
    - ä½¿ç”¨ "On hand (current)" å’Œ "On hand (new)" åˆ—
    """
    headers = [
        'Handle', 'Title',
        'Option1 Name', 'Option1 Value',
        'Option2 Name', 'Option2 Value',
        'Option3 Name', 'Option3 Value',
        'SKU', 'HS Code', 'COO',
        'Location', 'Bin name',
        'Incoming (not editable)', 'Unavailable (not editable)',
        'Committed (not editable)', 'Available (not editable)',
        'On hand (current)', 'On hand (new)'
    ]

    handle = product.handle or re.sub(r'[^a-z0-9]+', '-', product.title.lower()).strip('-')
    option1_name = product.options[0].get('name', 'Title') if product.options else 'Title'
    option2_name = product.options[1].get('name', '') if len(product.options) > 1 else ''
    option3_name = product.options[2].get('name', '') if len(product.options) > 2 else ''

    rows = []
    for variant in product.variants:
        common = {
            'Handle': handle,
            'Title': product.title,
            'Option1 Name': option1_name,
            'Option1 Value': variant.option1 or 'Default Title',
            'Option2 Name': option2_name,
            'Option2 Value': variant.option2 or '',
            'Option3 Name': option3_name,
            'Option3 Value': variant.option3 or '',
            'SKU': variant.sku or '',
            'HS Code': '',
            'COO': '',
        }
        # ä¸»ä»“åº“è¡Œï¼šè®¾ç½®åº“å­˜æ•°é‡
        main_row = {
            **common,
            'Location': location_name,
            'Bin name': '',
            'Incoming (not editable)': '0',
            'Unavailable (not editable)': '0',
            'Committed (not editable)': '0',
            'Available (not editable)': '0',
            'On hand (current)': '0',
            'On hand (new)': str(quantity),
        }
        rows.append(main_row)
        # AutoDS è¡Œï¼šnot stocked
        autods_row = {
            **common,
            'Location': AUTODS_LOCATION_NAME,
            'Bin name': '',
            'Incoming (not editable)': 'not stocked',
            'Unavailable (not editable)': 'not stocked',
            'Committed (not editable)': 'not stocked',
            'Available (not editable)': 'not stocked',
            'On hand (current)': 'not stocked',
            'On hand (new)': '',
        }
        rows.append(autods_row)

    try:
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            writer.writerows(rows)
        log_info(f"åº“å­˜CSVå·²ç”Ÿæˆ: {output_path} ({len(product.variants)} ä¸ªå˜ä½“, æ•°é‡={quantity})")
        return True
    except Exception as e:
        log_error(f"åº“å­˜CSVå†™å…¥å¤±è´¥: {e}")
        return False


def sync_inventory(inventory_csv_file: str) -> bool:
    """
    å®Œæ•´çš„åº“å­˜åŒæ­¥æµç¨‹ï¼š
    1. ä¸‹è½½Cookie + è·å–CSRF Token
    2. InventoryStagedUploads â†’ è·å–GCSä¸Šä¼ å‡­è¯
    3. ä¸Šä¼ åº“å­˜CSVåˆ°GCS
    4. InventoryImportCreate â†’ åˆ›å»ºå¯¼å…¥ä»»åŠ¡
    5. InventoryImportSubmit â†’ æäº¤å¯¼å…¥
    6. JobPoller â†’ è½®è¯¢ç­‰å¾…å®Œæˆ
    """
    for attempt in range(1, 3):
        log_info(f"ğŸ“¦ åº“å­˜åŒæ­¥ï¼ˆç¬¬{attempt}æ¬¡å°è¯•ï¼‰: {os.path.basename(inventory_csv_file)}")
        if _do_inventory_sync(inventory_csv_file):
            return True
        if attempt < 2:
            log_warning("åº“å­˜åŒæ­¥å¤±è´¥ï¼Œ10ç§’åé‡è¯•...")
            time.sleep(10)
    return False


def _do_inventory_sync(inventory_csv_file: str) -> bool:
    """æ‰§è¡Œåº“å­˜åŒæ­¥çš„å…·ä½“é€»è¾‘"""
    cookie_list = download_cookies()
    if not cookie_list:
        return False

    from requests.cookies import RequestsCookieJar
    jar = RequestsCookieJar()
    for c in cookie_list:
        domain = c.get('domain', '')
        path   = c.get('path', '/')
        jar.set(c['name'], c['value'], domain=domain, path=path)

    session = requests.Session()
    session.cookies = jar

    cookies_dict     = {c['name']: c['value'] for c in cookie_list}
    session_token    = cookies_dict.get('_shopify_s', '')
    multitrack_token = cookies_dict.get('_shopify_y', '')

    file_path = Path(inventory_csv_file)
    file_size = file_path.stat().st_size
    filename  = file_path.name
    log_info(f"åº“å­˜æ–‡ä»¶: {filename}ï¼Œå¤§å°: {file_size} bytes")

    # è·å– CSRF Token
    csrf_token = _get_csrf_token_selenium(cookie_list)
    if not csrf_token:
        return False

    page_view_token = str(uuid.uuid4())

    # åº“å­˜æ“ä½œçš„å…¬å…± headers
    inv_headers = {
        'accept': 'application/json',
        'accept-language': 'zh-CN,zh;q=0.9',
        'apollographql-client-name': 'core',
        'cache-control': 'no-cache,no-store,must-revalidate,max-age=0',
        'content-type': 'application/json',
        'origin': 'https://admin.shopify.com',
        'referer': (f'https://admin.shopify.com/store/{STORE_ID}/products/inventory'
                    f'?location_id={INVENTORY_LOCATION_ID}'),
        'user-agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                       '(KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'),
        'shopify-proxy-api-enable': 'true',
        'target-manifest-route-id': 'products:inventory:list',
        'target-pathname': '/store/:storeHandle/products/inventory',
        'target-slice': 'inventory-section',
        'x-csrf-token': csrf_token,
    }

    client_context = {
        "page_view_token": page_view_token,
        "client_route_handle": "products:inventory:list",
        "client_pathname": f"/store/{STORE_ID}/products/inventory",
        "client_normalized_pathname": "/store/:storeHandle/products/inventory",
        "shopify_session_token": session_token,
        "shopify_multitrack_token": multitrack_token
    }

    # â”€â”€ æ­¥éª¤1: InventoryStagedUploads â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log_info("ğŸ“¤ åº“å­˜æ­¥éª¤1: InventoryStagedUploadsï¼ˆè·å–GCSä¸Šä¼ å‡­è¯ï¼‰")
    stage_url = (
        f"https://admin.shopify.com/api/operations/"
        f"dafbde9e8213fb109b67860a344cd72657293731daa8abb55ddc0245a477716c/"
        f"InventoryStagedUploads/shopify/{STORE_ID}"
    )
    stage_payload = {
        "operationName": "InventoryStagedUploads",
        "variables": {
            "input": [{
                "filename": filename,
                "mimeType": "text/csv",
                "fileSize": str(file_size),
                "httpMethod": "POST",
                "resource": "INVENTORY_IMPORT"
            }]
        },
        "extensions": {"client_context": client_context}
    }

    try:
        resp = session.post(stage_url, headers=inv_headers, json=stage_payload, timeout=30)
        if resp.status_code != 200:
            log_error(f"InventoryStagedUploads å¤±è´¥: {resp.status_code} {resp.text[:300]}")
            return False

        result = resp.json()
        if 'errors' in result:
            log_error(f"InventoryStagedUploads GraphQL é”™è¯¯: {result['errors']}")
            return False

        staged = result['data']['stagedUploadsCreate']['stagedTargets'][0]
        upload_url  = staged['url']
        parameters  = staged['parameters']
        log_info("âœ… åº“å­˜GCSå‡­è¯è·å–æˆåŠŸ")
    except Exception as e:
        log_error(f"InventoryStagedUploads å¼‚å¸¸: {e}")
        return False

    # â”€â”€ æ­¥éª¤2: ä¸Šä¼ åº“å­˜CSVåˆ°GCS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log_info("ğŸ“¤ åº“å­˜æ­¥éª¤2: ä¸Šä¼ CSVåˆ°Google Cloud Storage")
    try:
        files_data = {}
        for param in parameters:
            files_data[param['name']] = (None, param['value'])

        with open(inventory_csv_file, 'rb') as f:
            files_data['file'] = (filename, f, 'text/csv')
            upload_headers = {
                'accept': '*/*',
                'origin': 'https://admin.shopify.com',
                'referer': 'https://admin.shopify.com/',
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            up_resp = requests.post(upload_url, headers=upload_headers,
                                    files=files_data, timeout=60)

        if up_resp.status_code in [200, 201, 204]:
            log_info("âœ… åº“å­˜CSVä¸Šä¼ åˆ°GCSæˆåŠŸ")
        else:
            log_error(f"åº“å­˜GCSä¸Šä¼ å¤±è´¥: {up_resp.status_code} {up_resp.text[:300]}")
            return False
    except Exception as e:
        log_error(f"åº“å­˜GCSä¸Šä¼ å¼‚å¸¸: {e}")
        return False

    # ä» GCS å‚æ•°ä¸­æå– staged key
    staged_key = None
    for param in parameters:
        if param.get('name') == 'key':
            staged_key = param['value']
            break

    if not staged_key:
        log_error("æœªæ‰¾åˆ°åº“å­˜ GCS staged key")
        return False

    # â”€â”€ æ­¥éª¤3: InventoryImportCreate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log_info(f"ğŸ“¥ åº“å­˜æ­¥éª¤3: InventoryImportCreateï¼Œstaged_key: {staged_key}")
    create_url = (
        f"https://admin.shopify.com/api/operations/"
        f"8d2fcb60da9f65b5f03a0f9efed1ae09b64e237405a6aabab8c530247ce79a49/"
        f"InventoryImportCreate/shopify/{STORE_ID}"
    )
    idempotency_key_create = str(uuid.uuid4())
    create_payload = {
        "operationName": "InventoryImportCreate",
        "variables": {
            "url": staged_key,
            "idempotencyKey": idempotency_key_create
        },
        "extensions": {"client_context": client_context}
    }

    try:
        resp = session.post(create_url, headers=inv_headers, json=create_payload, timeout=30)
        log_info(f"InventoryImportCreate å“åº”: HTTP {resp.status_code}")

        if resp.status_code != 200:
            log_error(f"InventoryImportCreate å¤±è´¥: {resp.status_code}")
            return False

        result = resp.json()
        if 'errors' in result:
            log_error(f"InventoryImportCreate GraphQL é”™è¯¯: {result['errors']}")
            return False

        try:
            import_gid = result['data']['inventoryImportCreate']['inventoryImport']['id']
        except (KeyError, TypeError) as e:
            log_error(f"æ— æ³•æå– InventoryImport ID: {e}ï¼Œå“åº”: {json.dumps(result)[:500]}")
            return False

        log_info(f"âœ… InventoryImportCreate æˆåŠŸï¼ŒImport ID: {import_gid}")

    except Exception as e:
        log_error(f"InventoryImportCreate å¼‚å¸¸: {e}")
        return False

    # â”€â”€ æ­¥éª¤4: InventoryImportSubmit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log_info(f"ğŸ“¤ åº“å­˜æ­¥éª¤4: InventoryImportSubmitï¼ŒID: {import_gid}")
    submit_url = (
        f"https://admin.shopify.com/api/operations/"
        f"e1cbb128d9f0abd1c1b35dc85ab7ae7718944c96e5a4538b945acca1a707bd95/"
        f"InventoryImportSubmit/shopify/{STORE_ID}"
    )
    idempotency_key_submit = str(uuid.uuid4())
    submit_payload = {
        "operationName": "InventoryImportSubmit",
        "variables": {
            "id": import_gid,
            "idempotencyKey": idempotency_key_submit
        },
        "extensions": {"client_context": client_context}
    }

    job_id = None
    try:
        resp = session.post(submit_url, headers=inv_headers, json=submit_payload, timeout=30)
        log_info(f"InventoryImportSubmit å“åº”: HTTP {resp.status_code}")

        if resp.status_code != 200:
            log_error(f"InventoryImportSubmit å¤±è´¥: {resp.status_code}")
            return False

        result = resp.json()
        if 'errors' in result:
            log_error(f"InventoryImportSubmit GraphQL é”™è¯¯: {result['errors']}")
            return False

        # æå– Job ID ç”¨äºè½®è¯¢
        try:
            job_data = result.get('data', {}).get('inventoryImportSubmit', {})
            job_id = job_data.get('job', {}).get('id')
        except (KeyError, TypeError, AttributeError):
            pass

        log_info("âœ… InventoryImportSubmit æˆåŠŸï¼åº“å­˜å¯¼å…¥å·²æäº¤")

    except Exception as e:
        log_error(f"InventoryImportSubmit å¼‚å¸¸: {e}")
        return False

    # â”€â”€ æ­¥éª¤5: JobPollerï¼ˆè½®è¯¢ç­‰å¾…å®Œæˆï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if job_id:
        log_info(f"â³ åº“å­˜æ­¥éª¤5: JobPoller è½®è¯¢ï¼ŒJob ID: {job_id}")
        _poll_inventory_job(session, inv_headers, job_id, csrf_token)
    else:
        log_info("æœªè·å–åˆ° Job IDï¼Œè·³è¿‡è½®è¯¢ï¼ˆåº“å­˜å¯¼å…¥å·²æäº¤ï¼Œå°†åœ¨åå°å¼‚æ­¥å®Œæˆï¼‰")

    return True


def _poll_inventory_job(session: requests.Session, headers: dict,
                         job_id: str, csrf_token: str,
                         max_polls: int = 20, interval: int = 5):
    """
    è½®è¯¢ Shopify å¼‚æ­¥ Job çŠ¶æ€ï¼Œç›´åˆ°å®Œæˆæˆ–è¶…æ—¶ã€‚
    """
    poller_base_url = (
        f"https://admin.shopify.com/api/operations/"
        f"e1593abda1eb0795fd588f8374f0f642659c1252872a4117c0ffd5e1db328980/"
        f"JobPoller/shopify/{STORE_ID}"
    )

    variables_json = json.dumps({"id": job_id})
    params = parse.urlencode({
        "operationName": "JobPoller",
        "variables": variables_json
    })
    poll_url = f"{poller_base_url}?{params}"

    for i in range(1, max_polls + 1):
        time.sleep(interval)
        try:
            resp = session.get(poll_url, headers=headers, timeout=15)
            if resp.status_code != 200:
                log_warning(f"JobPoller ç¬¬{i}æ¬¡ HTTP {resp.status_code}")
                continue

            result = resp.json()
            job_data = result.get('data', {}).get('job', {})
            done = job_data.get('done', False)

            if done:
                log_info(f"âœ… åº“å­˜å¯¼å…¥ Job å·²å®Œæˆï¼ˆç¬¬{i}æ¬¡è½®è¯¢ï¼‰")
                return
            else:
                log_info(f"â³ åº“å­˜å¯¼å…¥è¿›è¡Œä¸­... ({i}/{max_polls})")
        except Exception as e:
            log_warning(f"JobPoller ç¬¬{i}æ¬¡å¼‚å¸¸: {e}")

    log_warning(f"JobPoller è¾¾åˆ°æœ€å¤§è½®è¯¢æ¬¡æ•° ({max_polls})ï¼Œåº“å­˜å¯¼å…¥å¯èƒ½ä»åœ¨åå°è¿›è¡Œ")


# ============================================================
# å•ä»»åŠ¡å¤„ç†ï¼ˆæµ‹è¯•ç”¨ï¼‰
# ============================================================

def process_one_task(analyzer: ZhipuImageAnalyzer) -> str:
    """
    å¤„ç†å•æ¡ä»»åŠ¡
    è¿”å›å€¼: 'success' / 'failed' / 'skipped'
    """
    task = fetch_one_task()
    if not task:
        log_info("æš‚æ— å¾…å¤„ç†ä»»åŠ¡ï¼Œé€€å‡ºã€‚")
        return 'skipped'

    keer_product_id      = task.get('keer_product_id')
    client_product_url   = task.get('client_product_url')
    client_product_image = task.get('client_product_image')
    quotation_result     = task.get('quotation_result')

    log_info(f"--- å¼€å§‹å¤„ç†ä»»åŠ¡: {keer_product_id} ---")
    log_info(f"å•†å“URL: {client_product_url}")

    # è§£æä»·æ ¼ï¼ˆåŸå§‹ä¸ºæ¬§å…ƒï¼ŒÃ—1.2 è½¬ä¸ºç¾å…ƒï¼‰
    price = parse_price_from_quotation(quotation_result)
    if price is None:
        log_warning("ä»·æ ¼è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä»·æ ¼ 0.0")
        price = 0.0
    price_eur = price
    price = round(price * 1.2, 2)
    log_info(f"è§£æä»·æ ¼: â‚¬{price_eur} â†’ ${price}ï¼ˆÃ—1.2 EURâ†’USDï¼‰")

    # æŠ“å–å•†å“
    scraper = ShopifyScraper()
    product = scraper.fetch(client_product_url)
    if not product:
        log_error("å•†å“æŠ“å–å¤±è´¥")
        feedback_task_status(keer_product_id, 2)
        return 'failed'

    log_info(f"å•†å“æ ‡é¢˜: {product.title} | å˜ä½“: {len(product.variants)} | å›¾ç‰‡: {len(product.images)}")

    # AIåˆ†ç±»
    category = None
    if client_product_image:
        log_info("æ­£åœ¨è¯†åˆ«å•†å“åˆ†ç±»...")
        category = get_product_category(analyzer, client_product_image)
    log_info(f"å•†å“åˆ†ç±»: {category or 'æœªè®¾ç½®'}")

    # ç”ŸæˆCSV
    csv_dir = os.path.join(LOG_DIR, 'csv')
    os.makedirs(csv_dir, exist_ok=True)
    csv_path = os.path.join(csv_dir, f"shopify_import_{keer_product_id}.csv")

    if not generate_shopify_csv(product, price, category, csv_path):
        log_error("CSVç”Ÿæˆå¤±è´¥")
        feedback_task_status(keer_product_id, 2)
        return 'failed'

    # ä¸Šä¼ CSV
    upload_ok = upload_csv_to_shopify(csv_path)

    if upload_ok:
        # â”€â”€ åº“å­˜åŒæ­¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        log_info(f"äº§å“å¯¼å…¥æˆåŠŸï¼Œç­‰å¾… {INVENTORY_WAIT_SECONDS} ç§’ååŒæ­¥åº“å­˜...")
        time.sleep(INVENTORY_WAIT_SECONDS)

        inventory_csv_path = os.path.join(csv_dir, f"inventory_{keer_product_id}.csv")
        if generate_inventory_csv(product, INVENTORY_LOCATION_NAME, inventory_csv_path,
                                   quantity=INVENTORY_QUANTITY):
            inv_ok = sync_inventory(inventory_csv_path)
            if inv_ok:
                log_info(f"âœ… åº“å­˜åŒæ­¥æˆåŠŸ: {keer_product_id}")
            else:
                log_warning(f"âš ï¸ åº“å­˜åŒæ­¥å¤±è´¥ï¼ˆä¸å½±å“äº§å“å¯¼å…¥çŠ¶æ€ï¼‰: {keer_product_id}")
        else:
            log_warning(f"âš ï¸ åº“å­˜CSVç”Ÿæˆå¤±è´¥: {keer_product_id}")

        feedback_task_status(keer_product_id, 1)
        log_info(f"âœ… ä»»åŠ¡å®Œæˆ: {keer_product_id}")
        return 'success'
    else:
        feedback_task_status(keer_product_id, 2)
        log_error(f"âŒ ä»»åŠ¡å¤±è´¥: {keer_product_id}")
        return 'failed'


# ============================================================
# ç¨‹åºå…¥å£ï¼ˆå•ä»»åŠ¡æµ‹è¯•æ¨¡å¼ï¼‰
# ============================================================

def run_forever(task_interval: int = 10, key_refresh_hours: int = 1):
    """
    æ— é™å¾ªç¯è¿è¡Œ Shopify è‡ªåŠ¨ä¸Šæ¶ä»»åŠ¡ã€‚
    24å°æ—¶ä¸é—´æ–­ä»æ•°æ®åº“æ‹‰å–ä»»åŠ¡å¹¶å¤„ç†ã€‚

    å‚æ•°:
        task_interval:      æ¯æ¬¡ä»»åŠ¡ä¹‹é—´çš„ç­‰å¾…ç§’æ•°ï¼ˆé»˜è®¤10ç§’ï¼‰
        key_refresh_hours:  ZhipuAIå¯†é’¥åˆ·æ–°é—´éš”ï¼ˆå°æ—¶ï¼Œé»˜è®¤1å°æ—¶ï¼‰
    """
    _ensure_log_dir()

    log_info("åˆå§‹åŒ– ZhipuAI å¯†é’¥...")
    if not init_global_api_keys():
        log_error("ZhipuAI å¯†é’¥åˆå§‹åŒ–å¤±è´¥ï¼Œ60ç§’åé‡è¯•...")
        time.sleep(60)
        if not init_global_api_keys():
            log_error("ZhipuAI å¯†é’¥åˆå§‹åŒ–äºŒæ¬¡å¤±è´¥ï¼Œé€€å‡º")
            return

    analyzer = ZhipuImageAnalyzer()

    print("=" * 60)
    print("ğŸš€ Shopify è‡ªåŠ¨ä¸Šæ¶ â€” æ— é™å¾ªç¯æ¨¡å¼å·²å¯åŠ¨")
    print(f"   ä»»åŠ¡é—´éš”: {task_interval}ç§’")
    print(f"   å¯†é’¥åˆ·æ–°: æ¯{key_refresh_hours}å°æ—¶")
    print(f"   æ—¥å¿—ç›®å½•: {LOG_DIR}")
    print("=" * 60)
    log_info("æ— é™å¾ªç¯æ¨¡å¼å·²å¯åŠ¨")

    task_count = 0
    success_count = 0
    fail_count = 0
    last_key_refresh = time.time()

    while True:
        try:
            # å®šæ—¶åˆ·æ–° ZhipuAI å¯†é’¥
            if time.time() - last_key_refresh > key_refresh_hours * 3600:
                log_info("â° å®šæ—¶åˆ·æ–° ZhipuAI å¯†é’¥...")
                if refresh_api_keys():
                    log_info("âœ… å¯†é’¥åˆ·æ–°æˆåŠŸ")
                else:
                    log_warning("âš ï¸ å¯†é’¥åˆ·æ–°å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æ—§å¯†é’¥")
                last_key_refresh = time.time()

            # å¤„ç†ä¸€æ¡ä»»åŠ¡
            result = process_one_task(analyzer)

            if result == 'success':
                task_count += 1
                success_count += 1
                log_info(f"ğŸ“Š ç´¯è®¡: å¤„ç†{task_count}æ¡, æˆåŠŸ{success_count}, å¤±è´¥{fail_count}")
            elif result == 'failed':
                task_count += 1
                fail_count += 1
                log_info(f"ğŸ“Š ç´¯è®¡: å¤„ç†{task_count}æ¡, æˆåŠŸ{success_count}, å¤±è´¥{fail_count}")
            else:
                # skipped â€” æ²¡æœ‰æ–°ä»»åŠ¡
                pass

            time.sleep(task_interval)

        except KeyboardInterrupt:
            log_info("ğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            print(f"\næœ€ç»ˆç»Ÿè®¡: å¤„ç†{task_count}æ¡, æˆåŠŸ{success_count}, å¤±è´¥{fail_count}")
            break
        except Exception as e:
            log_error(f"ğŸ’¥ å¾ªç¯ä¸­æœªé¢„æœŸå¼‚å¸¸: {e}")
            log_error(traceback.format_exc())
            log_info(f"30ç§’åç»§ç»­è¿è¡Œ...")
            time.sleep(30)


if __name__ == "__main__":
    run_forever()
